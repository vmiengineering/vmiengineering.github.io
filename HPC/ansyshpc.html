<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<title data-rh="true">Ansys Parallel Processing | VMI Engineering</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://vmiengineering.github.io/HPC/ansyshpc"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Ansys Parallel Processing | VMI Engineering"><meta data-rh="true" name="description" content="ANSYS Academic Research HPC Workgroup 128"><meta data-rh="true" property="og:description" content="ANSYS Academic Research HPC Workgroup 128"><link data-rh="true" rel="icon" href="/img/logo.ico"><link data-rh="true" rel="canonical" href="https://vmiengineering.github.io/HPC/ansyshpc"><link data-rh="true" rel="alternate" href="https://vmiengineering.github.io/HPC/ansyshpc" hreflang="en"><link data-rh="true" rel="alternate" href="https://vmiengineering.github.io/HPC/ansyshpc" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.0fe5130e.css">
<link rel="preload" href="/assets/js/runtime~main.d0ead02c.js" as="script">
<link rel="preload" href="/assets/js/main.8496385f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.ico" alt="VMI Engineering" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.ico" alt="VMI Engineering" class="themedImage_W2Cr themedImage--dark_oUvU"></div></a><a class="navbar__item navbar__link navbar__link--active" href="/">VMI ENGINEERING</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Documentation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/Forms/ece_3d_print_request">Forms</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/Software/ansys">Software</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/HPC/ansyshpc">HPC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/HPC/ansyshpc">Ansys Parallel Processing</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/Tutorials/licenseserver">Tutorials</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/contact">Support and Contact</a></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Xlws" aria-label="breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a class="breadcrumbs__link breadcrumbsItemLink_e5ie" href="/">üè†</a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link breadcrumbsItemLink_e5ie">HPC</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><a class="breadcrumbs__link breadcrumbsItemLink_e5ie" href="/HPC/ansyshpc">Ansys Parallel Processing</a></li></ul></nav><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Ansys Parallel Processing</h1></header><p>ANSYS Academic Research HPC Workgroup 128<br>
<!-- -->ANSYS Academic Research CFD (25 Tasks)<br>
<!-- -->ANSYS Academic Research Mechanical and CFD (25 Tasks)  </p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>DOWNLOAD</h5></div><div class="admonition-content"><p><a href="https://vmi.box.com/s/4lk2qvd13kig96ikwdrjz8bt162ic65m" target="_blank" rel="noopener noreferrer">Download PDF Version Here</a></p></div></div><div class="admonition admonition-danger alert alert--danger"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>IMPORTANT</h5></div><div class="admonition-content"><p><a href="/contact">Please contact me if you need access to any other referenced guides in the Parallel Processing guide</a></p><p>MOST OF YOU ARE PROBABLY NEEDING INFORMATION ON HOW TO UTILIZE PARALLEL PROCESSING ON A SINGLE WORKSTATION.  PLEASE LOOK OVER THE DOCUMENTATION FOR <a href="#sharedmemory"><strong>SHARED-MEMORY PARALLEL PROCESSING (CHAPTER 2).</strong></a>  MUCH OF THE SETTINGS AND IF IT WILL EVEN BENEFIT YOU IS BASED SOLELY ON THE SCOPE OF YOUR PROJECT AND WHAT YOU ARE DOING.</p></div></div><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>LICENSE</h5></div><div class="admonition-content"><p><strong>VMI has 128 HPC Workgroup (1 task) concurrent licenses available</strong></p><p>Please Note: ANSYS uses physical cores vs virtual cores (hyperthreading)  <strong>DO NOT EXCEED THE PHYSICAL CORE COUNT</strong></p></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="chapter-1-overview-of-parallel-processing">Chapter 1: Overview of Parallel Processing<a class="hash-link" href="#chapter-1-overview-of-parallel-processing" title="Direct link to heading">‚Äã</a></h2><p>Solving a large model with millions of DOFs or a medium-sized model with nonlinearities that needs
many iterations to reach convergence can require many CPU hours. To decrease simulation time, ANSYS,
Inc. offers different parallel processing options that increase the model-solving power of ANSYS products
by using multiple processors (also known as cores). The following three parallel processing capabilities
are available:</p><ul><li><a href="#sharedmemory">Shared-memory parallel processing (Shared-Memory ANSYS) (Single [Multicore/Processor] Workstation)</a></li><li><a href="#distributedmemory">Distributed-memory parallel processing (Distributed ANSYS) (Multiple machine configuration)</a></li><li><a href="#gpu">GPU acceleration (a type of shared-memory parallel processing</a></li></ul><p>Multicore processors, and thus the ability to use parallel processing, are now widely available on all
computer systems, from laptops to high-end servers. <strong>The benefits of parallel processing are compelling
but are also among the most misunderstood</strong>. This chapter explains the two types of parallel processing
available in ANSYS and also discusses the use of GPUs (considered a form of shared-memory parallel
processing) and how they can further accelerate the time to solution.</p><p><strong>Currently, the default scheme is to use two cores with distributed-memory parallelism. For many of the
computations involved in a simulation, the speedups obtained from parallel processing are nearly linear
as the number of cores is increased, making very effective use of parallel processing. However, the total
benefit (measured by elapsed time) is problem dependent and is influenced by many different factors.</strong></p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>IMPORTANT</h5></div><div class="admonition-content"><p>No matter what form of parallel processing is used, the maximum benefit attained will always be limited
by the amount of work in the code that cannot be parallelized. If just 20 percent of the runtime is spent
in nonparallel code, the maximum theoretical speedup is only 5X, assuming the time spent in parallel
code is reduced to zero. However, parallel processing is still an essential component of any HPC system;
by reducing wall clock elapsed time, it provides significant value when performing simulations.</p></div></div><blockquote><p><strong>Distributed ANSYS, shared-memory ANSYS, and GPU acceleration can require HPC licenses. You can use
up to four CPU cores or a combination of four CPUs and GPUs without using any HPC licenses. Additional
licenses will be needed to run with more than four. See HPC Licensing (p. 3 in PDF) for more information.</strong></p></blockquote><h3 class="anchor anchorWithStickyNavbar_mojV" id="11-parallel-processing-terminology">1.1 Parallel Processing Terminology<a class="hash-link" href="#11-parallel-processing-terminology" title="Direct link to heading">‚Äã</a></h3><p>It is important to fully understand the terms we use, both relating to our software and to the physical
hardware. The terms shared-memory ANSYS and Distributed ANSYS refer to our software offerings, which
run on shared-memory or distributed-memory hardware configurations. The term GPU accelerator capability
refers to our software offering which allows the program to take advantage of certain GPU
(graphics processing unit) hardware to accelerate the speed of the solver computations.</p><div class="tabs-container"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LplD tabs__item--active">1.1.1 Hardware Terminology</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LplD">1.1.2 Software Terminology</li></ul><div class="margin-vert--md"><div role="tabpanel"><p>The following terms describe the hardware configurations used for parallel processing:</p><p><strong>Shared-memory hardware</strong></p><p>This term refers to a physical hardware configuration in which a
single shared-memory address space is accessible by multiple CPU
cores; each CPU core &quot;shares&quot; the memory with the other cores.
A common example of a shared-memory system is a Windows
desktop machine or workstation with one or two multicore processors.</p><p><strong>Distributed-memory hardware</strong></p><p>This term refers to a physical hardware configuration in which
multiple machines are connected together on a network (that is,
a cluster). Each machine on the network (that is, each compute
node on the cluster) has its own memory address space. Communication
between machines is handled by interconnects (Gigabit
Ethernet, Infiniband, etc.).</p><p>Virtually all clusters involve both shared-memory and distributedmemory
hardware. Each compute node on the cluster typically
contains at least two or more CPU cores, which means there is a
shared-memory environment within a compute node. The distributed-
memory environment requires communication between the
compute nodes involved in the cluster.</p><p><strong>GPU hardware</strong></p><p>A graphics processing unit (GPU) is a specialized microprocessor
that off-loads and accelerates graphics rendering from the microprocessor.
Their highly parallel structure makes GPUs more effective
than general-purpose CPUs for a range of complex algorithms. In
a personal computer, a GPU on a dedicated video card is more
powerful than a GPU that is integrated on the motherboard.</p><p><strong>Head compute node</strong></p><p>In a Distributed ANSYS run, the machine or node on which the
master process runs (that is, the machine on which the job is
launched). The head compute node should not be confused with
the host node in a Windows cluster environment. The host node
typically schedules multiple applications and jobs on a cluster,
but does not typically run the application.</p></div><div role="tabpanel" hidden=""><p><strong>Software Terminology</strong></p><p>The following terms describe our software offerings for parallel processing:</p><p><strong>Shared-memory ANSYS</strong></p><p>This term refers to running across multiple cores on a single machine
(for example, a desktop workstation or a single compute
node of a cluster). Shared-memory parallelism is invoked, which
allows each core involved to share data (or memory) as needed
to perform the necessary parallel computations.When run within
a shared-memory architecture, most computations in the solution
phase and many pre- and postprocessing operations are performed
in parallel. For more information, see Using Shared-Memory ANSYS
(p. 5).</p><p><strong>Distributed ANSYS</strong> </p><p>This term refers to running across multiple cores on a single machine
(for example, a desktop workstation or a single compute
node of a cluster) or across multiple machines (for example, a
cluster). Distributed-memory parallelism is invoked, and each core
communicates data needed to perform the necessary parallel
computations through the use of MPI (Message Passing Interface)
software.With Distributed ANSYS, all computations in the solution
phase are performed in parallel (including the stiffness matrix
generation, linear equation solving, and results calculations). Preand
postprocessing do not make use of the distributed-memory
parallel processing; however, these steps can make use of sharedmemory
parallelism. See Using Distributed ANSYS (p. 17) for more
details.</p><p><strong>GPU accelerator capability</strong> </p><p>This capability takes advantage of the highly parallel architecture
of the GPU hardware to accelerate the speed of solver computations
and, therefore, reduce the time required to complete a simulation.
Some computations of certain equation solvers can be
off-loaded from the CPU(s) to the GPU, where they are often executed
much faster. The CPU core(s) will continue to be used for
all other computations in and around the equation solvers. For
more information, see GPU Accelerator Capability (p. 9).</p><p><strong>Master process</strong> </p><p>The first process launched on the head compute node in a Distributed
ANSYS run.</p><p><strong>Worker process</strong> </p><p>A Distributed ANSYS process other than the master process.</p><p>Shared-memory ANSYS can only be run on shared-memory hardware. However, Distributed ANSYS
can be run on both shared-memory hardware or distributed-memory hardware.While both forms of
hardware can achieve a significant speedup with Distributed ANSYS, only running on distributedmemory
hardware allows you to take advantage of increased resources (for example, available memory
and disk space, as well as memory and I/O bandwidths) by using multiple machines. The GPU accelerator
capability can be used with either shared-memory ANSYS or Distributed ANSYS.</p></div></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="12-hpc-licensing">1.2 HPC Licensing<a class="hash-link" href="#12-hpc-licensing" title="Direct link to heading">‚Äã</a></h3><p>ANSYS, Inc. offers the following high performance computing license options:</p><ul><li><p><strong>ANSYS HPC</strong> - These physics-neutral licenses can be used to run a single analysis across multiple
processors (cores).</p></li><li><p><strong>ANSYS HPC Packs</strong> - These physics-neutral licenses share the same characteristics of the ANSYS HPC
licenses, but are combined into predefined packs to give you greater value and scalability.</p></li></ul><p>For detailed information on these HPC license options, see HPC Licensing in the ANSYS Licensing Guide.</p><p>The HPC license options cannot be combined with each other in a single solution; for example, you
cannot use both ANSYS HPC and ANSYS HPC Packs in the same analysis solution.</p><p>The order in which HPC licenses are used is specified by your user license preferences setting. See
Specify Product Order in the ANSYS Licensing Guide for more information on setting user license product
order.</p><p>You can choose a particular HPC license by using the Preferred Parallel Feature command line option.
The format is <code>ansys211 -ppf &lt;license feature name&gt;</code>, where <code>&lt;license feature name&gt;</code>
is the name of the HPC license option that you want to use. This option forces Mechanical APDL to use
the specified license feature for the requested number of parallel cores or GPUs. If the license feature
is entered incorrectly or the license feature is not available, a license failure occurs.</p><p>Both Distributed ANSYS and shared-memory ANSYS allow you to use four CPU cores without using any
HPC licenses. ANSYS HPC licenses add cores to this base functionality, while the ANSYS HPC Pack licenses
function independently of the four included cores.</p><p>In a similar way, you can use up to four CPU cores and GPUs combined without any HPC licensing (for
example, one CPU and three GPUs). The combined number of CPU cores and GPUs used cannot exceed
the task limit allowed by your specific license configuration.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="chapter-2-using-shared-memory-ansys-">Chapter 2: Using Shared-Memory ANSYS <a name="sharedmemory"></a><a class="hash-link" href="#chapter-2-using-shared-memory-ansys-" title="Direct link to heading">‚Äã</a></h2><p>When running a simulation, the solution time is typically dominated by three main parts: <em>the time spent
to create the element matrices and form the global matrices, the time to solve the linear system of
equations, and the time spent calculating derived quantities (such as stress and strain) and other requested
results for each element.</em></p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>IMPORTANT</h5></div><div class="admonition-content"><p>Shared-memory ANSYS can run a solution over multiple cores on a single machine.When using sharedmemory
parallel processing, you can reduce each of the three main parts of the overall solution time
by using multiple cores. However, this approach is often limited by the memory bandwidth; <strong>you typically
see very little reduction in solution time beyond four cores.</strong></p></div></div><p>The main program functions that run in parallel on shared-memory hardware are:</p><ul><li>Solvers such as the Sparse, PCG, ICCG, Block Lanczos, PCG Lanczos, Supernode, and Subspace running
over multiple processors but sharing the same memory address. These solvers typically have limited
scalability when used with shared-memory parallelism. In general, very little reduction in time occurs
when using more than four cores.</li><li>Forming element matrices and load vectors.</li><li>Computing derived quantities and other requested results for each element.</li><li>Pre- and postprocessing functions such as graphics, selecting, sorting, and other data and compute
intensive operations.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="21-activating-parallel-processing-in-a-shared-memory-architecture">2.1 Activating Parallel Processing in a Shared-Memory Architecture<a class="hash-link" href="#21-activating-parallel-processing-in-a-shared-memory-architecture" title="Direct link to heading">‚Äã</a></h3><ol><li><p><strong>By default, shared-memory ANSYS uses two cores and does not require any HPC licenses. Additional
HPC licenses are required to run with more than four cores.</strong> Several HPC license options are available.
See HPC Licensing (p. 3) for more information.</p></li><li><p>Open the Mechanical APDL Product Launcher:</p></li></ol><ul><li>Windows: <code>Start &gt;Programs &gt;ANSYS 2021 R1 &gt;Mechanical APDL Product Launcher</code></li><li>Linux: <code>launcher211</code></li></ul><ol start="3"><li><p>Select the correct environment and license.</p></li><li><p>Go to the <code>High Performance Computing Setup tab</code>. Select <code>Use Shared-Memory Parallel (SMP).</code>
Specify the number of cores to use.</p></li><li><p>Alternatively, you can specify the number of cores to use via the -np command line option:</p></li></ol><p><code>ansys211 -smp -np N</code></p><p>where <em>N</em> represents the number of cores to use.</p><p><strong>For large multiprocessor servers, ANSYS, Inc. recommends setting <em>N</em> to a value no higher than the
number of available cores minus one. For example, on an eight-core system, set N to 7. However,
on multiprocessor workstations, you may want to use all available cores to minimize the total
solution time. The program automatically limits the maximum number of cores used to be less
than or equal to the number of physical cores on the machine. This is done to avoid running the
program on virtual cores (for example, by means of hyperthreading), which typically results in poor
per-core performance. For optimal performance, consider closing down all other applications before
launching ANSYS.</strong></p><p>If you have more than one HPC license feature, you can use the -ppf command line option to
specify which HPC license to use for the parallel run. See HPC Licensing (p. 3) for more information.</p><ol start="6"><li><p>If working from the launcher, click Run to launch ANSYS.</p></li><li><p>Set up and run your analysis as you normally would.</p></li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="211-system-specific-considerations">2.1.1 System Specific Considerations<a class="hash-link" href="#211-system-specific-considerations" title="Direct link to heading">‚Äã</a></h3><p>For shared-memory parallel processing, the number of cores that the program uses is limited to the
lesser of one of the following:</p><ul><li>The number of ANSYS HPC licenses available (plus the first four cores which do not require any licenses)</li><li>The number of cores indicated via the -np command line argument</li><li>The actual number of cores available</li></ul><p>You can specify multiple settings for the number of cores to use during a session. However, ANSYS,
Inc. recommends that you issue the <strong>/CLEAR</strong> command before resetting the number of cores for
subsequent analyses.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="22-troubleshooting">2.2 Troubleshooting<a class="hash-link" href="#22-troubleshooting" title="Direct link to heading">‚Äã</a></h3><p>This section describes problems which you may encounter while using shared-memory parallel processing
as well as methods for overcoming these problems. Some of these problems are specific to a particular
system, as noted.</p><div class="tabs-container"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LplD tabs__item--active">Job Failes with SIGTERM signal (Linux Only)</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LplD">Poor Speedup or No Speedup</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LplD">Different Results Relative to a Single Core</li></ul><div class="margin-vert--md"><div role="tabpanel"><p>Occasionally, when running on Linux, a simulation may fail with the following message: ‚Äúprocess
killed (SIGTERM)‚Äù. This typically occurs when computing the solution and means that the system
has killed the ANSYS process. The two most common occurrences are (1) ANSYS is using too much
of the hardware resources and the system has killed the ANSYS process or (2) a user has manually
killed the ANSYS job (that is, <strong>kill -9</strong> system command). Users should check the size of job they are
running in relation to the amount of physical memory on the machine. Most often, decreasing the
model size or finding a machine with more RAM will result in a successful run.</p></div><div role="tabpanel" hidden="">As more cores are utilized, the runtimes are generally expected to decrease. The biggest relative gains are typically achieved when using two cores compared to using a single core.When significant speedups are not seen as additional cores are used, the reasons may involve both hardware and software issues. These include, but are not limited to, the following situations.<p><strong>HARDWARE</strong></p><p><strong>Oversubscribing hardware</strong>   </p><p>In a multiuser environment, this could mean that more physical cores are being used by ANSYS
simulations than are available on the machine. It could also mean that hyperthreading is activated.
Hyperthreading typically involves enabling extra virtual cores, which can sometimes allow software
programs to more effectively use the full processing power of the CPU. However, for computeintensive
programs such as ANSYS, using these virtual cores rarely provides a significant reduction
in runtime. Therefore, it is recommended you disable hyperthreading; if hyperthreading is enabled,
it is recommended you do not exceed the number of physical cores.</p><p><strong>Lack of memory bandwidth</strong>
On some systems, using most or all of the available cores can result in a lack of memory
bandwidth. This lack of memory bandwidth can affect the overall scalability of the ANSYS
software.</p><p><strong>Dynamic Processor Speeds</strong></p><p>Many new CPUs have the ability to dynamically adjust the clock speed at which they operate
based on the current workloads. Typically, when only a single core is being used the clock
speed can be significantly higher than when all of the CPU cores are being utilized. This
can have a negative effect on scalability as the per-core computational performance can
be much higher when only a single core is active versus the case when all of the CPU cores
are active.</p><p><strong>Software</strong></p><p><strong>Simulation includes non-supported features</strong>    </p><p>The shared- and distributed-memory parallelisms work to speed up certain compute-intensive
operations in <strong>/PREP7, /SOLU</strong> and <strong>/POST1</strong>. However, not all operations are parallelized. If a
particular operation that is not parallelized dominates the simulation time, then using additional
cores will not help achieve a faster runtime.  </p><p><strong>Simulation has too few DOF (degrees of freedom)</strong></p><p>Some analyses (such as transient analyses) may require long compute times, not because
the number of DOF is large, but because a large number of calculations are performed (that
is, a very large number of time steps). Generally, if the number of DOF is relatively small,
parallel processing will not significantly decrease the solution time. Consequently, for small
models with many time steps, parallel performance may be poor because the model size
is too small to fully utilize a large number of cores.    </p><p><strong>I/O cost dominates solution time</strong></p><p>For some simulations, the amount of memory required to obtain a solution is greater than
the physical memory (that is, RAM) available on the machine. In these cases, either virtual
memory (that is, hard disk space) is used by the operating system to hold the data that
would otherwise be stored in memory, or the equation solver writes extra files to the disk
to store data. In both cases, the extra I/O done using the hard drive can significantly affect
performance, making the I/O performance the main bottleneck to achieving optimal performance.
In these cases, using additional cores will typically not result in a significant reduction
in overall time to solution.</p></div><div role="tabpanel" hidden=""><p>Shared-memory parallel processing occurs in various preprocessing, solution, and postprocessing
operations. Operational randomness and numerical round-off inherent to parallelism can cause
slightly different results between runs on the same machine using the same number of cores or
different numbers of cores. This difference is often negligible. However, in some cases the difference
is appreciable. This sort of behavior is most commonly seen on nonlinear static or transient analyses
which are numerically unstable. The more numerically unstable the model is, the more likely the
convergence pattern or final results will differ as the number of cores used in the simulation is
changed.</p><p>With shared-memory parallelism, you can use the <strong>PSCONTROL</strong> command to control which operations
actually use parallel behavior. For example, you could use this command to show that the element
matrix generation running in parallel is causing a nonlinear job to converge to a slightly different
solution each time it runs (even on the same machine with no change to the input data). This can
help isolate parallel computations which are affecting the solution while maintaining as much other
parallelism as possible to continue to reduce the time to solution.</p></div></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="chapter-3-gpu-accelerator-capability-">Chapter 3: GPU Accelerator Capability <a name="gpu"></a><a class="hash-link" href="#chapter-3-gpu-accelerator-capability-" title="Direct link to heading">‚Äã</a></h2><p>In an effort to provide faster performance during solution, Mechanical APDL supports offloading key
solver computations onto graphics cards to accelerate those computations. Only high-end graphics
cards, the ones with the most amount of cores and memory, can be used to accelerate the solver
computations. For details on which GPU devices are supported and the corresponding driver versions,
see the GPU requirements outlined in the Windows Installation Guide and the Linux Installation Guide.</p><p>It is important to understand that a GPU does not replace the CPU core(s) on which a simulation typically
runs. One or more CPU cores must be used to run the Mechanical APDL program. The GPUs are used
in support of the CPU to process certain calculations. The CPU continues to handle most operations
and will automatically offload some of the time-intensive parallel operations performed by certain
equation solvers. These parallel solver operations can usually be performed much faster on the highly
parallel architecture of a GPU, thus accelerating these solvers and reducing the overall time to solution.</p><p>GPU acceleration can be used with both shared-memory parallel processing (shared-memory ANSYS)
and distributed-memory parallel processing (Distributed ANSYS). In shared-memory ANSYS, one or
multiple GPU accelerator devices can be utilized during solution. In Distributed ANSYS, one or multiple
GPU accelerator devices per machine or compute node can be utilized during solution.</p><p>As an example, when using Distributed ANSYS on a cluster involving eight compute nodes with each
compute node having two supported GPU accelerator devices, either a single GPU per node (a total of
eight GPU cards) or two GPUs per node (a total of sixteen GPU cards) can be used to accelerate the
solution. The GPU accelerator device usage must be consistent across all compute nodes. For example,
if running a simulation across all compute nodes, it is not possible to use one GPU for some compute
nodes and zero or two GPUs for the other compute nodes.</p><p>On machines containing multiple GPU accelerator devices, the program automatically selects the GPU
accelerator device (or devices) to be used for the simulation. The program cannot detect if a GPU device
is currently being used by other software, including another Mechanical APDL simulation. Therefore, in
a multiuser environment, users should be careful not to oversubscribe the GPU accelerator devices by
simultaneously launching multiple simulations that attempt to use the same GPU (or GPUs) to accelerate
the solution. For more information, see Oversubscribing GPU Hardware (p. 14) in the troubleshooting
discussion.</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>IMPORTANT</h5></div><div class="admonition-content"><p>The GPU accelerator capability is only supported on the Windows 64-bit and Linux x64 platforms.</p><p>You can use up to four GPUs and CPUs combined without any HPC licensing (for example, one CPU
and three GPUs). To use more than four, you need one or more ANSYS HPC licenses or ANSYS HPC Pack
licenses. For more information see HPC Licensing in the ANSYS Licensing Guide.</p></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="31-activating-the-gpu-accelerator-capability">3.1 Activating the GPU Accelerator Capability<a class="hash-link" href="#31-activating-the-gpu-accelerator-capability" title="Direct link to heading">‚Äã</a></h3><p>Following is the general procedure to use the GPU accelerator capability:</p><ol><li><p>Before activating the GPU accelerator capability, you must have at least one GPU card installed
with the proper driver level. You may also need some type of HPC license; see HPC licensing
for details.</p></li><li><p>Open the Mechanical APDL Product Launcher.</p></li></ol><ul><li>Windows: <code>Start &gt;Programs &gt;ANSYS 2021 R1 &gt;Mechanical APDL Product Launcher</code></li><li>Linux: <code>launcher211</code></li></ul><ol start="3"><li><p>Select the correct environment and license.</p></li><li><p>Go to the <strong>High Performance Computing Setup tab</strong>, select a GPU device from the <strong>GPU Accelerator</strong>
drop-down menu, and specify the number of GPU accelerator devices.</p></li><li><p>Alternatively, you can activate the GPU accelerator capability via the <em>-acc</em> command line option:
<code>ansys211 -acc nvidia -na N</code>
The <em>-na</em> command line option followed by a number <em>(N)</em> indicates the number of GPU accelerator
devices to use per machine or compute node. If only the <em>-acc</em> option is specified, the
program uses a single GPU device per machine or compute node by default (that is, <em>-na 1</em>).</p></li></ol><p>If you have more than one HPC license feature, you can use the <em>-ppf</em> command line option
to specify which HPC license to use for the parallel run. See HPC Licensing (p. 3) for more information.</p><ol start="6"><li><p>If working from the launcher, click <em>Run</em> to launch Mechanical APDL.</p></li><li><p>Set up and run your analysis as you normally would.</p></li></ol><p>With the GPU accelerator capability, the acceleration obtained by using the parallelism on the GPU
hardware occurs only during the solution operations. Operational randomness and numerical round-off
inherent to any parallel algorithm can cause slightly different results between runs on the same machine
when using or not using the GPU hardware to accelerate the simulation.</p><p>The <strong>ACCOPTION</strong> command can also be used to control activation of the GPU accelerator capability.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="32-supported-analysis-types-and-features">3.2 Supported Analysis Types and Features<a class="hash-link" href="#32-supported-analysis-types-and-features" title="Direct link to heading">‚Äã</a></h3><p>Some analysis types and features are not supported by the GPU accelerator capability. Supported
functionality also depends on the specified GPU hardware. The following section gives general guidelines
on what is and is not supported.</p><p>These are not comprehensive lists, but represent major features and capabilities found in the Mechanical
APDL program.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="321-nvidia-gpu-hardware">3.2.1 NVIDIA GPU Hardware<a class="hash-link" href="#321-nvidia-gpu-hardware" title="Direct link to heading">‚Äã</a></h3><p>This section lists analysis capabilities that are supported by the GPU accelerator capability when using
NVIDIA GPU cards.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="supported-analysis-types">Supported Analysis Types<a class="hash-link" href="#supported-analysis-types" title="Direct link to heading">‚Äã</a></h4><p>The following analysis types are supported and will use the GPU to accelerate the solution.</p><ul><li>Static linear or nonlinear analyses using the sparse, PCG, or JCG solver.</li><li>Buckling analyses using the Block Lanczos or subspace eigensolver.</li><li>Modal analyses using the Block Lanczos, subspace, PCG Lanczos, QR damped, unsymmetric,
or damped eigensolver.</li><li>Harmonic analyses using the full method and the sparse solver.</li><li>Transient linear or nonlinear analyses using the full method and the sparse, PCG, or JCG
solver.</li><li>Substructuring analyses, generation pass only, including the generation pass of component
mode synthesis (CMS) analyses.</li></ul><p>In situations where the analysis type is not supported by the GPU accelerator capability, the solution
will continue but GPU acceleration will not be used.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="performance-issues-for-some-solverhardware-combinations">Performance Issues for Some Solver/Hardware Combinations<a class="hash-link" href="#performance-issues-for-some-solverhardware-combinations" title="Direct link to heading">‚Äã</a></h3><p>When using the PCG or JCG solver, or the PCG Lanczos eigensolver, any of the recommended NVIDIA
GPU devices can be expected to achieve good performance.</p><p>When using the sparse solver or eigensolvers based on the sparse solver (for example, Block Lanczos
or subspace), only NVIDIA GPU devices with significant double precision performance (FP64) are
recommended in order to achieve good performance. For a list of these devices, see the Windows
Installation Guide and the Linux Installation Guide.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="shared-memory-parallel-behavior">Shared-Memory Parallel Behavior<a class="hash-link" href="#shared-memory-parallel-behavior" title="Direct link to heading">‚Äã</a></h3><p>For the sparse solver (and eigensolvers based on the sparse solver), if one or more GPUs are requested,
only a single GPU is used no matter how many are requested.</p><p>For the PCG and JCG solvers (and eigensolvers based on the PCG solver), all requested GPUs are
used.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="distributed-memory-parallel-behavior">Distributed-Memory Parallel Behavior<a class="hash-link" href="#distributed-memory-parallel-behavior" title="Direct link to heading">‚Äã</a></h3><p>For the sparse solver (and eigensolvers based on the sparse solver), if the number of GPUs exceeds
the number of processes (the -na value is greater than the -np value on the command line), the
number of GPUs used equals the -np value. If the number of GPUs is less than the number of
processes (-na is less than -np), all requested GPUs are used.
For the PCG and JCG solvers (and eigensolvers based on the PCG solver), if the number of GPUs
exceeds the number of processes (-na is greater than -np), all requested GPUs are used. If the
number of GPUs is less than the number of processes (-na is less than -np), all requested GPUs
are used.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="3212-supported-features">3.2.1.2 Supported Features<a class="hash-link" href="#3212-supported-features" title="Direct link to heading">‚Äã</a></h3><p>As the GPU accelerator capability currently only pertains to the equation solvers, virtually all features
and element types are supported when using this capability with the supported equation solvers
listed in Supported Analysis Types (p. 11). A few limitations exist and are listed below. In these
situations, the solution will continue but GPU acceleration will not be used (unless otherwise noted):</p><ul><li>Partial pivoting is activated when using the sparse solver. This most commonly occurs when
using current technology elements with mixed u-P formulation, Lagrange multiplier based
MPC184 elements, Lagrange multiplier based contact elements (TARGE169 through CONTA178),
or certain circuit elements (CIRCU94, CIRCU124).</li><li>The memory saving option is activated (MSAVE,ON) when using the PCG solver. In this
particular case, the MSAVE option is turned off and GPU acceleration is used.</li><li>Unsymmetric matrices when using the PCG solver.</li><li>A non-supported equation solver is used (for example, ICCG, etc.).</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="33-troubleshooting">3.3 Troubleshooting<a class="hash-link" href="#33-troubleshooting" title="Direct link to heading">‚Äã</a></h3><p>This section describes problems which you may encounter while using the GPU accelerator capability,
as well as methods for overcoming these problems. Some of these problems are specific to a particular
system, as noted.</p><p>NVIDIA GPUs support various compute modes (for example, Exclusive thread, Exclusive process). Only
the default compute mode is supported. Using other compute modes may cause the program to fail
to launch.</p><p>To list the GPU devices installed on the machine, set the <strong>ANSGPU_PRINTDEVICES</strong> environment variable
to a value of 1. The printed list may or may not include graphics cards used for display purposes, along
with any graphics cards used to accelerate your simulation.</p><ul><li><p><strong>NO DEVICES</strong></p><p>  Be sure that a recommended GPU device is properly installed and configured. Check the driver level
to be sure it is current or newer than the driver version supported for your particular device. (See
the GPU requirements outlined in the Windows Installation Guide and the Linux Installation Guide.)</p><p>  When using NVIDIA GPU devices, use of the CUDA_VISIBLE_DEVICES environment variable can block
some or all of the GPU devices from being visible to the program. Try renaming this environment
variable to see if the supported devices can be used.</p></li></ul><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Important</h5></div><div class="admonition-content"><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">On Windows, the use of Remote Desktop may disable the use of a GPU device. Launching</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Mechanical APDL through the ANSYS Remote Solve Manager (RSM) when RSM is installed</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">as a service may also disable the use of a GPU. In these two scenarios, the GPU Accelerator</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Capability cannot be used. Using the TCC (Tesla Compute Cluster) driver mode, if</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">applicable, can circumvent this restriction.</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></div></div><ul><li><p><strong>NO VALID DEVICES</strong></p><p>  A GPU device was detected, but it is not a recommended GPU device. Be sure that a recommended
GPU device is properly installed and configured. Check the driver level to be sure it is current or
newer than the supported driver version for your particular device. (See the GPU requirements
outlined in the Windows Installation Guide and the Linux Installation Guide.) Consider using the
ANSGPU_OVERRIDE environment variable to override the check for valid GPU devices.</p><p>  When using NVIDIA GPU devices, use of the CUDA_VISIBLE_DEVICES environment variable can block
some or all of the GPU devices from being visible to the program. Try renaming this environment
variable to see if the supported devices can be used.</p></li><li><p><strong>POOR ACCELERATION OR NO ACCELERATION</strong></p><p>  <strong>Simulation includes non-supported features</strong></p><p>  A GPU device will only accelerate certain portions of a simulation, mainly the solution time. If the
bulk of the simulation time is spent outside of solution, the GPU cannot have a significant effect
on the overall analysis time. Even if the bulk of the simulation is spent inside solution, you must be
sure that a supported equation solver is utilized during solution and that no unsupported options
are used. Messages are printed in the output to alert users when a GPU is being used, as well as
when unsupported options/features are chosen which deactivate the GPU accelerator capability.</p><p>  <strong>Simulation has too few DOF (degrees of freedom)</strong></p><p>  Some analyses (such as transient analyses) may require long compute times, not because the
number of DOF is large, but because a large number of calculations are performed (that is, a
very large number of time steps). Generally, if the number of DOF is relatively small, GPU acceleration
will not significantly decrease the solution time. Consequently, for small models with
many time steps, GPU acceleration may be poor because the model size is too small to fully
utilize a GPU.</p><p>  <strong>Simulation does not fully utilize the GPU</strong></p><p>  Only simulations that spend a lot of time performing calculations that are supported on a GPU
can expect to see significant speedups when a GPU is used. Only certain computations are
supported for GPU acceleration. Therefore, users should check to ensure that a high percentage
of the solution time was spent performing computations that could possibly be accelerated
on a GPU. This can be done by reviewing the equation solver statistics files as described below.
See Measuring Performance in the Performance Guide for more details on the equation solver
statistics files.</p><ul><li><p><em>PCG solver file</em>: The .PCS file contains statistics for the PCG iterative solver. You should
first check to make sure that the GPU was utilized by the solver. This can be done by
looking at the line which begins with: ‚ÄúNumber of cores used‚Äù. The string ‚ÄúGPU acceleration
enabled‚Äù will be added to this line if the GPU hardware was used by the solver. If
this string is missing, the GPU was not used for that call to the solver. Next, you should
study the elapsed times for both the ‚ÄúPreconditioner Factoring‚Äù and ‚ÄúMultiply With A22‚Äù
computations. GPU hardware is only used to accelerate these two sets of computations.
The wall clock (or elapsed) times for these computations are the areas of interest when
determining how much GPU acceleration is achieved.</p></li><li><p><em>Sparse solver files</em>: The .DSP file contains statistics for the sparse direct solver. You
should first check to make sure that the GPU was utilized by the solver. This can be
done by looking for the following line: ‚ÄúGPU acceleration activated‚Äù. This line will be
printed if the GPU hardware was used. If this line is missing, the GPU was not used for
that call to the solver. Next, you should check the percentage of factorization computations
(flops) which were accelerated on a GPU. This is shown by the line: ‚Äúpercentage
of GPU accelerated flops‚Äù. Also, you should look at the time to perform the matrix factorization,
shown by the line: ‚Äútime (cpu &amp; wall) for numeric factor‚Äù. GPU hardware is
only used to accelerate the matrix factor computations. These lines provide some indication
of how much GPU acceleration is achieved.</p></li><li><p><em>Eigensolver files</em>: The Block Lanczos and Subspace eigensolvers support the use of GPU
devices; however, no statistics files are written by these eigensolvers. The .PCS file is
written for the PCG Lanczos eigensolver and can be used as described above for the
PCG iterative solver.</p><p><strong>Using multiple GPU devices</strong></p><p>When using the sparse solver in a shared-memory parallel solution, it is expected that running
a simulation with multiple GPU devices will not improve performance compared to running
with a single GPU device. In a shared-memory parallel solution, the sparse solver can only make
use of one GPU device.</p><p><strong>Oversubscribing GPU hardware</strong></p><p>The program automatically determines which GPU devices to use. In a multiuser environment,
this could mean that one or more of the same GPUs are picked when multiple simulations are
run simultaneously, thus oversubscribing the hardware.</p></li><li><p>If only a single GPU accelerator device exists in the machine, then only a single user
should attempt to make use of it, much in the same way users should avoid oversubscribing
their CPU cores.</p></li><li><p>If multiple GPU accelerator devices exist in the machine, you can set the <strong>ANSGPU_DEVICE</strong>
environment variable, in conjunction with the <strong>ANSGPU_PRINTDEVICES</strong> environment
variable mentioned above, to specify which particular GPU accelerator devices to use
during the solution.</p><p>For example, consider a scenario where <strong>ANSGPU_PRINTDEVICES</strong> shows that four GPU
devices are available with device ID values of 1, 3, 5, and 7 respectively, and only the
second and third devices are supported for GPU acceleration. To select only the second
supported GPU device, set <strong>ANSGPU_DEVICE</strong> = 5. To select the first and second supported
GPU devices, set <strong>ANSGPU_DEVICE</strong> = 3:5.</p><p><strong>Solver/hardware combination</strong></p><p>When using NVIDIA GPU devices, some solvers may not achieve good performance on certain
devices. For more information, see Performance Issue for Some Solver/Hardware Combinations
(p. 11).</p></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="chapter-4-using-distributed-ansys">Chapter 4: Using Distributed ANSYS<a name="distributedmemory"></a><a class="hash-link" href="#chapter-4-using-distributed-ansys" title="Direct link to heading">‚Äã</a></h2><p>When running a simulation, the solution time is typically dominated by three main parts: the time spent
to create the element matrices and form the global matrices or global systems of equations, the time
to solve the linear system of equations, and the time spent calculating derived quantities (such as stress
and strain) and other requested results for each element.</p><p>The distributed-memory parallelism offered via Distributed ANSYS allows the entire solution phase to
run in parallel, including the stiffness matrix generation, linear equation solving, and results calculations.
As a result, a simulation using distributed-memory parallel processing usually achieves much faster
solution times than a similar run performed using shared-memory parallel processing (p. 5), particularly
at higher core counts.</p><p>Distributed ANSYS can run a solution over multiple cores on a single machine or on multiple machines
(that is, a cluster). It automatically decomposes the model into smaller domains, transfers the domains
to each core, solves each domain simultaneously, and creates a complete solution to the model. The
memory and disk space required to complete the solution can also be distributed over multiple machines.
By utilizing all of the resources of a cluster (computing power, RAM, memory and I/O bandwidth), distributed-
memory parallel processing can be used to solve very large problems much more efficiently
compared to the same simulation run on a single machine.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="distributed-ansys-behavior">Distributed ANSYS Behavior<a class="hash-link" href="#distributed-ansys-behavior" title="Direct link to heading">‚Äã</a></h4><p>Distributed ANSYS works by launching multiple ANSYS processes on either a single machine or on
multiple machines (as specified by one of the following command line options: -np, -machines, or -
mpifile). The machine that the distributed run is launched from is referred to as the head compute
node, and the other machines are referred to as the compute nodes. The first process launched on the
head compute node is referred to as the master process; all other processes are referred to as the
worker processes.</p><p>Each Distributed ANSYS process is essentially a running process of shared-memory ANSYS. These processes
are launched through the specified MPI software layer. The MPI software allows each Distributed ANSYS
process to communicate, or exchange data, with the other processes involved in the distributed simulation.</p><p>Distributed ANSYS does not currently support all of the analysis types, elements, solution options, etc.
that are available with shared-memory ANSYS (see Supported Features (p. 30)). In some cases, Distributed
ANSYS stops the analysis to avoid performing an unsupported action. If this occurs, you must launch
shared-memory ANSYS to perform the simulation. In other cases, Distributed ANSYS will automatically
disable the distributed-memory parallel processing capability and perform the operation using sharedmemory
parallelism. This disabling of the distributed-memory parallel processing can happen at various
levels in the program.</p><p>The master process handles the inputting of commands as well as all of the pre- and postprocessing
actions. Only certain commands (for example, the <strong>SOLVE</strong> command and supporting commands such
as <strong>/SOLU, FINISH, /EOF, /EXIT</strong>, and so on) are communicated to the worker processes for execution.</p><p>Therefore, outside of the SOLUTION processor (<strong>/SOLU</strong>), Distributed ANSYS behaves very similar to
shared-memory ANSYS. The master process works on the entire model during these pre- and postprocessing
steps and may use shared-memory parallelism to improve performance of these operations.
During this time, the worker processes wait to receive new commands from the master process.</p><p>Once the <strong>SOLVE</strong> command is issued, it is communicated to the worker processes and all Distributed
ANSYS processes become active. At this time, the program makes a decision as to which mode to use
when computing the solution. In some cases, the solution will proceed using only a distributed-memory
parallel (DMP) mode. In other cases, similar to pre- and postprocessing, the solution will proceed using
only a shared-memory parallel (SMP) mode. In a few cases, a mixed mode may be implemented which
tries to use as much distributed-memory parallelism as possible for maximum performance. These three
modes are described further below.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="pure-dmp-mode">Pure DMP Mode<a class="hash-link" href="#pure-dmp-mode" title="Direct link to heading">‚Äã</a></h4><p>The simulation is fully supported by Distributed ANSYS, and distributed-memory parallelism is used
throughout the solution. This mode typically provides optimal performance in Distributed ANSYS.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="mixed-mode">Mixed Mode<a class="hash-link" href="#mixed-mode" title="Direct link to heading">‚Äã</a></h4><p>The simulation involves a particular set of computations that is not supported by Distributed ANSYS.
Examples include certain equation solvers and remeshing due to mesh nonlinear adaptivity. In
these cases, distributed-memory parallelism is used throughout the solution, except for the unsupported
set of computations.When that step is reached, the worker processes in Distributed ANSYS
simply wait while the master process uses shared-memory parallelism to perform the computations.
After the computations are finished, the worker processes continue to compute again until the
entire solution is completed.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="pure-smp-mode">Pure SMP Mode<a class="hash-link" href="#pure-smp-mode" title="Direct link to heading">‚Äã</a></h4><p>The simulation involves an analysis type or feature that is not supported by Distributed ANSYS. In
this case, distributed-memory parallelism is disabled at the onset of the solution, and sharedmemory
parallelism is used instead. The worker processes in Distributed ANSYS are not involved
at all in the solution but simply wait while the master process uses shared-memory parallelism to
compute the entire solution.</p><p>When using shared-memory parallelism inside of Distributed ANSYS (in mixed mode or SMP mode, including
all pre- and postprocessing operations), the master process will not use more cores on the head
compute node than the total cores you specify to be used for the Distributed ANSYS solution. This is
done to avoid exceeding the requested CPU resources or the requested number of licenses.</p><p>The following table shows which steps, including specific equation solvers, can be run in parallel using
shared-memory ANSYS and Distributed ANSYS.</p><p>Table 4.1 Parallel Capability in Shared-Memory and Distributed ANSYS</p><table><thead><tr><th>Solvers/ Feature</th><th align="center">Shared-Memory ANSYS</th><th align="center">Distributed ANSYS</th></tr></thead><tbody><tr><td>Sparse</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>PCG</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>ICCG</td><td align="center">Y</td><td align="center">Y <!-- -->[1]</td></tr><tr><td>JCG</td><td align="center">Y</td><td align="center">Y <!-- -->[1][2]</td></tr><tr><td>QMR <!-- -->[3]</td><td align="center">Y</td><td align="center">Y <!-- -->[1]</td></tr><tr><td>Block Lanczos eigensolver</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>PCG Lanczos eigensolver</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>Supernode eigensolver</td><td align="center">Y</td><td align="center">Y <!-- -->[1]</td></tr><tr><td>Subspace eigensolver</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>Unsymmectric eigensolver</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>Damped eigensolver</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>QRDAMP eigensolver</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>Element formulation, results calculation</td><td align="center">Y</td><td align="center">Y</td></tr><tr><td>Graphics and other pre- and postprocessing</td><td align="center">Y</td><td align="center">Y <!-- -->[1]</td></tr></tbody></table><ol><li><p>This solver/operation only runs in mixed mode.</p></li><li><p>For static analyses and transient analyses using the full method (<strong>TRNOPT</strong>,FULL), the JCG equation
solver runs in pure DMP mode only when the matrix is symmetric. Otherwise, it runs in SMP mode.</p></li><li><p>The QMR solver only supports 1 core in SMP mode and in mixed mode.</p></li></ol><p>The maximum number of cores allowed in a Distributed ANSYS analysis is currently set at 8192. Therefore,
you can run Distributed ANSYS using anywhere from 2 to 8192 cores (assuming the appropriate HPC
licenses are available) for each individual job. Performance results vary widely for every model when
using any form of parallel processing. For every model, there is a point where using more cores does
not significantly reduce the overall solution time. Therefore, it is expected that most models run in
Distributed ANSYS can not efficiently make use of hundreds or thousands of cores.</p><p>Files generated by Distributed ANSYS are named <em>Jobnamen.ext</em>, where n is the process number. (See
Differences in General Behavior (p. 32) for more information.) The master process is always numbered
0, and the worker processes are 1, 2, etc.When the solution is complete and you issue the <strong>FINISH</strong>
command in the SOLUTION processor, Distributed ANSYS combines all <em>Jobnamen.RST</em> files into a
single <em>Jobname.RST</em> file, located on the head compute node. Other files, such as <em>.MODE, .ESAV,
.EMAT</em>, etc., may be combined as well upon finishing a distributed solution. (See Differences in Postprocessing
(p. 37) for more information.)</p><p>The remaining sections explain how to configure your environment to run Distributed ANSYS, how to
run a Distributed ANSYS analysis, and what features and analysis types are supported in Distributed
ANSYS. You should read these sections carefully and fully understand the process before attempting
to run a distributed analysis. The proper configuration of your environment and the installation and
configuration of the appropriate MPI software are critical to successfully running a distributed analysis.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="41-configuring-distributed-ansys">4.1 Configuring Distributed ANSYS<a class="hash-link" href="#41-configuring-distributed-ansys" title="Direct link to heading">‚Äã</a></h3><p><strong>To run Distributed ANSYS on a single machine, no additional setup is required.</strong></p><p>To run an analysis with Distributed ANSYS on a cluster, some configuration is required as described in
the following sections:</p><p>4.1.1. Prerequisites for Running Distributed ANSYS
4.1.2. Setting Up the Cluster Environment for Distributed ANSYS</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="411-prerequisites-for-running-distributed-ansys">4.1.1 Prerequisites for Running Distributed ANSYS<a class="hash-link" href="#411-prerequisites-for-running-distributed-ansys" title="Direct link to heading">‚Äã</a></h4><p>Whether you are running on a single machine or multiple machines, the following condition is true:</p><ul><li>By default, Distributed ANSYS uses two cores and does not require any HPC licenses. Additional licenses
will be needed to run a distributed solution with more than four cores. Several HPC license
options are available. For more information, see HPC Licensing (p. 3) in the Parallel Processing
Guide (p. 1).</li></ul><p>If you are running on a single machine, there are no additional requirements for running a distributed
solution.</p><p>If you are running across multiple machines (for example, a cluster), your system must meet these
additional requirements to run a distributed solution.</p><ul><li>Homogeneous network: All machines in the cluster must be the same type, OS level, chip set, and
interconnects.</li><li>You must be able to remotely log in to all machines, and all machines in the cluster must have
identical directory structures (including the ANSYS 2021 R1 installation, MPI installation, and
working directories). Do not change or rename directories after you&#x27;ve launched ANSYS. For more
information, see Directory Structure Across Machines (p. 29) in the Parallel Processing Guide (p. 1).</li><li>All machines in the cluster must have ANSYS 2021 R1 installed, or must have an NFS mount to the
ANSYS 2021 R1 installation. If not installed on a shared file system, ANSYS 2021 R1 must be installed
in the same directory path on all systems.</li><li>All machines must have the same version of MPI software installed and running. The table below
shows the MPI software and version level supported for each platform.</li></ul><h4 class="anchor anchorWithStickyNavbar_mojV" id="4111-mpi-software">4.1.1.1 MPI Software<a class="hash-link" href="#4111-mpi-software" title="Direct link to heading">‚Äã</a></h4><p>The MPI software supported by Distributed ANSYS depends on the platform (see the table below).</p><p>The files needed to run Distributed ANSYS using Intel MPI, MS MPI, or Open MPI are included on
the installation media and are installed automatically when you install ANSYS 2021 R1. Therefore,
when running on a single machine (for example, a laptop, a workstation, or a single compute node
of a cluster) on Windows or Linux, or when running on a Linux cluster, no additional software is
needed. However, when running on multiple Windows machines you must use a cluster setup, and
you must install the MPI software separately (see Installing the Software (p. 21) later in this section).</p><p><strong>Table 4.2: Platforms and MPI Software</strong></p><table><thead><tr><th>Platform</th><th>MPI Software</th></tr></thead><tbody><tr><td>Linux</td><td>Intel MPI 2018.3.222</td></tr><tr><td></td><td>Open MPI 3.1.5a</td></tr><tr><td>Windows 10 (Single Machine)</td><td>Intel MPI 2018.3.210</td></tr><tr><td></td><td>MS MPI v10.1.12</td></tr><tr><td>Windows Server 2016 (Cluster)</td><td>Microsoft HPC Pack (MS MPI v10.1.12)b</td></tr></tbody></table><p>Mellanox OFED driver version 4.4 or higher is required.</p><p>If you are running Distributed ANSYS across multiple Windows machines, you must use Microsoft HPC Pack (MS MPI) and
the HPC Job Manager to start Distributed ANSYS (see Activating Distributed ANSYS (p. 25) ).</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="4112-installing-the-software">4.1.1.2 Installing the Software<a class="hash-link" href="#4112-installing-the-software" title="Direct link to heading">‚Äã</a></h4><p>Install ANSYS 2021 R1 following the instructions in the ANSYS, Inc. Installation Guide for your platform.
Be sure to complete the installation, including all required post-installation procedures.</p><p>To run Distributed ANSYS on a cluster, you must:</p><ul><li>Install ANSYS 2021 R1 on all machines in the cluster, in the exact same location on each
machine.</li><li>For Windows, you can use shared drives and symbolic links. Install ANSYS 2021 R1 on one
Windows machine (for example, C:\Program Files\ANSYS Inc\V211) and then share
that installation folder. On the other machines in the cluster, create a symbolic link (at
C:\Program Files\ANSYS Inc\V211) that points to the UNC path for the shared
folder. On Windows systems, you must use the Universal Naming Convention (UNC) for all
file and path names for Distributed ANSYS to work correctly.</li><li>For Linux, you can use the exported NFS file systems. Install ANSYS 2021 R1 on one Linux
machine (for example, at /ansys_inc/v211), and then export this directory. On the other
machines in the cluster, create an NFS mount from the first machine to the same local directory
(/ansys_inc/v211).</li></ul><h4 class="anchor anchorWithStickyNavbar_mojV" id="installing-mpi-software-on-windows">Installing MPI Software on Windows<a class="hash-link" href="#installing-mpi-software-on-windows" title="Direct link to heading">‚Äã</a></h4><p>You can install Intel MPI from the installation launcher by choosing <strong>Install MPI for ANSYS, Inc.
Parallel Processing</strong>. For installation instructions see:</p><p>Intel-MPI 2018.3.210 Installation Instructions in the ANSYS, Inc. Installation Guides</p><p>Microsoft MPI is installed and ready for use as part of the ANSYS 2021 R1 installation, but if you
require MS MPI on another machine, the installer can be found at C:\Program Files\ANSYS
Inc\V211\commonfiles\MPI\Microsoft\10.1.12498.18\Windows\MSMpiSetup.exe</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="microsoft-hpc-pack-windows-hpc-server-2016">Microsoft HPC Pack (Windows HPC Server 2016)<a class="hash-link" href="#microsoft-hpc-pack-windows-hpc-server-2016" title="Direct link to heading">‚Äã</a></h4><p>You must complete certain post-installation steps before running Distributed ANSYS on a Microsoft
HPC Server 2016 system. The post-installation instructions provided below assume that Microsoft
HPC Server 2016 and Microsoft HPC Pack (which includes MS MPI) are already installed on your
system. The post-installation instructions can be found in the following README files:</p><p><code>Program Files\ANSYS Inc\V211\commonfiles\MPI\WindowsHPC\README.mht</code>
or
<code>Program Files\ANSYS Inc\V211\commonfiles\MPI\WindowsHPC\README.docx</code></p><p>Microsoft HPC Pack examples are also located in <em>Program Files\ANSYS Inc\V211\commonfiles\
MPI\WindowsHPC</em>. Jobs are submitted to the Microsoft HPC Job Manager either from the
command line or the Job Manager GUI.</p><p>To submit a job via the GUI, go to <strong>Start&gt; All Programs&gt; Microsoft HPC Pack&gt; HPC Job Manager</strong>.
Then click on <strong>Create New Job from Description File.</strong></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="412-setting-up-the-cluster-environment-for-distributed-ansys">4.1.2. Setting up the Cluster Environment for Distributed ANSYS<a class="hash-link" href="#412-setting-up-the-cluster-environment-for-distributed-ansys" title="Direct link to heading">‚Äã</a></h4><p>After you&#x27;ve ensured that your cluster meets the prerequisites and you have ANSYS 2021 R1 and the
correct version of MPI installed, you need to configure your distributed environment using the following
procedure.</p><ol><li>Obtain the machine name for each machine on the cluster.</li></ol><ul><li><p><strong>Windows 10 and Windows Server 2016:</strong></p><p>From the Start menu, pick Settings &gt;System &gt;About. The full computer name is listed
under PC Name. Note the name of each machine (not including the domain).</p></li><li><p><strong>Linux:</strong>
Type <code>hostname</code> on each machine in the cluster. Note the name of each machine.</p></li></ul><ol start="2"><li><strong>Linux only</strong>: First determine if the cluster uses the secure shell (ssh) or remote shell (rsh) protocol.</li></ol><ul><li><p>For ssh: Use the ssh-keygen command to generate a pair of authentication keys. Do not
enter a passphrase. Then append the new public key to the list of authorized keys on
each compute node in the cluster that you wish to use.</p></li><li><p>For rsh: Create a .rhosts file in the home directory. Add the name of each compute
node you wish to use on a separate line in the .rhosts file. Change the permissions of
the .rhost file by issuing: <strong>chmod 600 .rhosts</strong>. Copy this .rhosts file to the home
directory on each compute node in the cluster you wish to use.</p></li></ul><p>Verify communication between compute nodes on the cluster via ssh or rsh. You should not be
prompted for a password. If you are, correct this before continuing. For more information on
using ssh/rsh without passwords, search online for &quot;Passwordless SSH&quot; or &quot;Passwordless RSH&quot;, or
see the man pages for ssh or rsh.</p><ol start="3"><li><strong>Windows only</strong>: Verify that all required environment variables are properly set. If you followed
the post-installation instructions described above for Microsoft HPC Pack (Windows HPC Server),
these variables should be set automatically.</li></ol><p>On the head compute node, where ANSYS 2021 R1 is installed, check these variables:</p><p><strong>ANSYS211_DIR</strong>=C:\Program Files\ANSYS Inc\v211\ansys
<strong>ANSYSLIC_DIR</strong>=C:\Program Files\ANSYS Inc\Shared Files\Licensing</p><p>where <code>C:\Program Files\ANSYS Inc</code> is the location of the product install and <code>C:\Program
Files\ANSYS Inc\Shared Files\Licensing</code> is the location of the licensing install. If
your installation locations are different than these, specify those paths instead.</p><p>On Windows systems, you must use the Universal Naming Convention (UNC) for all ANSYS, Inc.
environment variables on the compute nodes for Distributed ANSYS to work correctly.</p><p>On the compute nodes, check these variables:</p><p><strong>ANSYS211_DIR</strong>=<!-- -->\<!-- -->head_node_machine_name\ANSYS Inc\v211\ansys
<strong>ANSYSLIC_DIR</strong>=<!-- -->\<!-- -->head_node_machine_name\ANSYS Inc\Shared Files\Licensing</p><ol start="4"><li>Windows only: Share out the ANSYS Inc directory on the head node with full permissions so
that the compute nodes can access it.</li></ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://vmiengineering.github.io/docs/HPC/ansyshpc.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/Software/visanalysis"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Visual Analysis</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/Tutorials/licenseserver"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">License Server Change</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#chapter-1-overview-of-parallel-processing" class="table-of-contents__link toc-highlight">Chapter 1: Overview of Parallel Processing</a><ul><li><a href="#11-parallel-processing-terminology" class="table-of-contents__link toc-highlight">1.1 Parallel Processing Terminology</a></li><li><a href="#12-hpc-licensing" class="table-of-contents__link toc-highlight">1.2 HPC Licensing</a></li></ul></li><li><a href="#chapter-2-using-shared-memory-ansys-" class="table-of-contents__link toc-highlight">Chapter 2: Using Shared-Memory ANSYS <a name="sharedmemory"></a></a><ul><li><a href="#21-activating-parallel-processing-in-a-shared-memory-architecture" class="table-of-contents__link toc-highlight">2.1 Activating Parallel Processing in a Shared-Memory Architecture</a></li><li><a href="#211-system-specific-considerations" class="table-of-contents__link toc-highlight">2.1.1 System Specific Considerations</a></li><li><a href="#22-troubleshooting" class="table-of-contents__link toc-highlight">2.2 Troubleshooting</a></li></ul></li><li><a href="#chapter-3-gpu-accelerator-capability-" class="table-of-contents__link toc-highlight">Chapter 3: GPU Accelerator Capability <a name="gpu"></a></a><ul><li><a href="#31-activating-the-gpu-accelerator-capability" class="table-of-contents__link toc-highlight">3.1 Activating the GPU Accelerator Capability</a></li><li><a href="#32-supported-analysis-types-and-features" class="table-of-contents__link toc-highlight">3.2 Supported Analysis Types and Features</a></li><li><a href="#321-nvidia-gpu-hardware" class="table-of-contents__link toc-highlight">3.2.1 NVIDIA GPU Hardware</a></li><li><a href="#performance-issues-for-some-solverhardware-combinations" class="table-of-contents__link toc-highlight">Performance Issues for Some Solver/Hardware Combinations</a></li><li><a href="#shared-memory-parallel-behavior" class="table-of-contents__link toc-highlight">Shared-Memory Parallel Behavior</a></li><li><a href="#distributed-memory-parallel-behavior" class="table-of-contents__link toc-highlight">Distributed-Memory Parallel Behavior</a></li><li><a href="#3212-supported-features" class="table-of-contents__link toc-highlight">3.2.1.2 Supported Features</a></li><li><a href="#33-troubleshooting" class="table-of-contents__link toc-highlight">3.3 Troubleshooting</a></li></ul></li><li><a href="#chapter-4-using-distributed-ansys" class="table-of-contents__link toc-highlight">Chapter 4: Using Distributed ANSYS<a name="distributedmemory"></a></a><ul><li><a href="#41-configuring-distributed-ansys" class="table-of-contents__link toc-highlight">4.1 Configuring Distributed ANSYS</a></li></ul></li></ul></div></div></div></div></main></div></div></div>
<script src="/assets/js/runtime~main.d0ead02c.js"></script>
<script src="/assets/js/main.8496385f.js"></script>
</body>
</html>