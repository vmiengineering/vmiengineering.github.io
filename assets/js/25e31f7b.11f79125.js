"use strict";(self.webpackChunkeng_test=self.webpackChunkeng_test||[]).push([[875],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return h}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(n),h=r,d=p["".concat(l,".").concat(h)]||p[h]||m[h]||o;return n?a.createElement(d,i(i({ref:t},u),{},{components:n})):a.createElement(d,i({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},8215:function(e,t,n){var a=n(7294);t.Z=function(e){var t=e.children,n=e.hidden,r=e.className;return a.createElement("div",{role:"tabpanel",hidden:n,className:r},t)}},5064:function(e,t,n){n.d(t,{Z:function(){return m}});var a=n(7294),r=n(9443);var o=function(){var e=(0,a.useContext)(r.Z);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e},i=n(6010),s="tabItem_1uMI",l="tabItemActive_2DSg";var c=37,u=39;var m=function(e){var t=e.lazy,n=e.block,r=e.defaultValue,m=e.values,p=e.groupId,h=e.className,d=o(),f=d.tabGroupChoices,g=d.setTabGroupChoices,y=(0,a.useState)(r),k=y[0],v=y[1],N=a.Children.toArray(e.children),b=[];if(null!=p){var S=f[p];null!=S&&S!==k&&m.some((function(e){return e.value===S}))&&v(S)}var w=function(e){var t=e.currentTarget,n=b.indexOf(t),a=m[n].value;v(a),null!=p&&(g(p,a),setTimeout((function(){var e,n,a,r,o,i,s,c;(e=t.getBoundingClientRect(),n=e.top,a=e.left,r=e.bottom,o=e.right,i=window,s=i.innerHeight,c=i.innerWidth,n>=0&&o<=c&&r<=s&&a>=0)||(t.scrollIntoView({block:"center",behavior:"smooth"}),t.classList.add(l),setTimeout((function(){return t.classList.remove(l)}),2e3))}),150))},P=function(e){var t,n;switch(e.keyCode){case u:var a=b.indexOf(e.target)+1;n=b[a]||b[0];break;case c:var r=b.indexOf(e.target)-1;n=b[r]||b[b.length-1]}null==(t=n)||t.focus()};return a.createElement("div",{className:"tabs-container"},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":n},h)},m.map((function(e){var t=e.value,n=e.label;return a.createElement("li",{role:"tab",tabIndex:k===t?0:-1,"aria-selected":k===t,className:(0,i.Z)("tabs__item",s,{"tabs__item--active":k===t}),key:t,ref:function(e){return b.push(e)},onKeyDown:P,onFocus:w,onClick:w},n)}))),t?(0,a.cloneElement)(N.filter((function(e){return e.props.value===k}))[0],{className:"margin-vert--md"}):a.createElement("div",{className:"margin-vert--md"},N.map((function(e,t){return(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==k})}))))}},9443:function(e,t,n){var a=(0,n(7294).createContext)(void 0);t.Z=a},1888:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return m},toc:function(){return p},default:function(){return d}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=n(5064),s=n(8215),l=["components"],c={title:"Ansys Parallel Processing"},u=void 0,m={unversionedId:"HPC/ansyshpc",id:"HPC/ansyshpc",isDocsHomePage:!1,title:"Ansys Parallel Processing",description:"Download PDF Version Here",source:"@site/docs/HPC/ansyshpc.md",sourceDirName:"HPC",slug:"/HPC/ansyshpc",permalink:"/HPC/ansyshpc",editUrl:"https://vmiengineering.github.io/docs/HPC/ansyshpc.md",version:"current",frontMatter:{title:"Ansys Parallel Processing"},sidebar:"tutorialSidebar",previous:{title:"Visual Analysis",permalink:"/Software/visanalysis"},next:{title:"License Server Change",permalink:"/licenseserver"}},p=[{value:"Chapter 1: Overview of Parallel Processing",id:"chapter-1-overview-of-parallel-processing",children:[{value:"1.1 Parallel Processing Terminology",id:"11-parallel-processing-terminology",children:[]},{value:"1.2 HPC Licensing",id:"12-hpc-licensing",children:[]}]},{value:'Chapter 2: Using Shared-Memory ANSYS <a name="sharedmemory"></a>',id:"chapter-2-using-shared-memory-ansys",children:[{value:"2.1 Activating Parallel Processing in a Shared-Memory Architecture",id:"21-activating-parallel-processing-in-a-shared-memory-architecture",children:[]},{value:"2.1.1 System Specific Considerations",id:"211-system-specific-considerations",children:[]},{value:"2.2 Troubleshooting",id:"22-troubleshooting",children:[]}]},{value:'Chapter 3: GPU Accelerator Capability <a name="gpu"></a>',id:"chapter-3-gpu-accelerator-capability",children:[{value:"3.1 Activating the GPU Accelerator Capability",id:"31-activating-the-gpu-accelerator-capability",children:[]},{value:"3.2 Supported Analysis Types and Features",id:"32-supported-analysis-types-and-features",children:[]},{value:"3.2.1 NVIDIA GPU Hardware",id:"321-nvidia-gpu-hardware",children:[]},{value:"Performance Issues for Some Solver/Hardware Combinations",id:"performance-issues-for-some-solverhardware-combinations",children:[]},{value:"Shared-Memory Parallel Behavior",id:"shared-memory-parallel-behavior",children:[]},{value:"Distributed-Memory Parallel Behavior",id:"distributed-memory-parallel-behavior",children:[]},{value:"3.2.1.2 Supported Features",id:"3212-supported-features",children:[]},{value:"3.3 Troubleshooting",id:"33-troubleshooting",children:[]}]},{value:'Chapter 4: Using Distributed ANSYS<a name="distributedmemory"></a>',id:"chapter-4-using-distributed-ansys",children:[{value:"4.1 Configuring Distributed ANSYS",id:"41-configuring-distributed-ansys",children:[]}]}],h={toc:p};function d(e){var t=e.components,n=(0,r.Z)(e,l);return(0,o.kt)("wrapper",(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"DOWNLOAD")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},(0,o.kt)("a",{parentName:"p",href:"https://vmi.box.com/s/4lk2qvd13kig96ikwdrjz8bt162ic65m"},"Download PDF Version Here")))),(0,o.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"IMPORTANT")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},(0,o.kt)("a",{parentName:"p",href:"/contact"},"Please contact me if you need access to any other referenced guides in the Parallel Processing guide")),(0,o.kt)("p",{parentName:"div"},"MOST OF YOU ARE PROBABLY NEEDING INFORMATION ON HOW TO UTILIZE PARALLEL PROCESSING ON A SINGLE WORKSTATION.  PLEASE LOOK OVER THE DOCUMENTATION FOR ",(0,o.kt)("a",{parentName:"p",href:"#sharedmemory"},(0,o.kt)("strong",{parentName:"a"},"SHARED-MEMORY PARALLEL PROCESSING (CHAPTER 2).")),"  MUCH OF THE SETTINGS AND IF IT WILL EVEN BENEFIT YOU IS BASED SOLELY ON THE SCOPE OF YOUR PROJECT AND WHAT YOU ARE DOING."))),(0,o.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"LICENSE")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},(0,o.kt)("strong",{parentName:"p"},"VMI has 128 HPC Workgroup (1 task) concurrent licenses available")),(0,o.kt)("p",{parentName:"div"},"Please Note: ANSYS uses physical cores vs virtual cores (hyperthreading)  ",(0,o.kt)("strong",{parentName:"p"},"DO NOT EXCEED THE PHYSICAL CORE COUNT")))),(0,o.kt)("h2",{id:"chapter-1-overview-of-parallel-processing"},"Chapter 1: Overview of Parallel Processing"),(0,o.kt)("p",null,"Solving a large model with millions of DOFs or a medium-sized model with nonlinearities that needs\nmany iterations to reach convergence can require many CPU hours. To decrease simulation time, ANSYS,\nInc. offers different parallel processing options that increase the model-solving power of ANSYS products\nby using multiple processors (also known as cores). The following three parallel processing capabilities\nare available:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#sharedmemory"},"Shared-memory parallel processing (Shared-Memory ANSYS) (Single [Multicore/Processor] Workstation)")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#distributedmemory"},"Distributed-memory parallel processing (Distributed ANSYS) (Multiple machine configuration)")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"#gpu"},"GPU acceleration (a type of shared-memory parallel processing"))),(0,o.kt)("p",null,"Multicore processors, and thus the ability to use parallel processing, are now widely available on all\ncomputer systems, from laptops to high-end servers. ",(0,o.kt)("strong",{parentName:"p"},"The benefits of parallel processing are compelling\nbut are also among the most misunderstood"),". This chapter explains the two types of parallel processing\navailable in ANSYS and also discusses the use of GPUs (considered a form of shared-memory parallel\nprocessing) and how they can further accelerate the time to solution."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Currently, the default scheme is to use two cores with distributed-memory parallelism. For many of the\ncomputations involved in a simulation, the speedups obtained from parallel processing are nearly linear\nas the number of cores is increased, making very effective use of parallel processing. However, the total\nbenefit (measured by elapsed time) is problem dependent and is influenced by many different factors.")),(0,o.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"IMPORTANT")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"No matter what form of parallel processing is used, the maximum benefit attained will always be limited\nby the amount of work in the code that cannot be parallelized. If just 20 percent of the runtime is spent\nin nonparallel code, the maximum theoretical speedup is only 5X, assuming the time spent in parallel\ncode is reduced to zero. However, parallel processing is still an essential component of any HPC system;\nby reducing wall clock elapsed time, it provides significant value when performing simulations."))),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("strong",{parentName:"p"},"Distributed ANSYS, shared-memory ANSYS, and GPU acceleration can require HPC licenses. You can use\nup to four CPU cores or a combination of four CPUs and GPUs without using any HPC licenses. Additional\nlicenses will be needed to run with more than four. See HPC Licensing (p. 3 in PDF) for more information."))),(0,o.kt)("h3",{id:"11-parallel-processing-terminology"},"1.1 Parallel Processing Terminology"),(0,o.kt)("p",null,"It is important to fully understand the terms we use, both relating to our software and to the physical\nhardware. The terms shared-memory ANSYS and Distributed ANSYS refer to our software offerings, which\nrun on shared-memory or distributed-memory hardware configurations. The term GPU accelerator capability\nrefers to our software offering which allows the program to take advantage of certain GPU\n(graphics processing unit) hardware to accelerate the speed of the solver computations."),(0,o.kt)(i.Z,{defaultValue:"hardware",values:[{label:"1.1.1 Hardware Terminology",value:"hardware"},{label:"1.1.2 Software Terminology",value:"software"}],mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"hardware",mdxType:"TabItem"},(0,o.kt)("p",null,"The following terms describe the hardware configurations used for parallel processing:"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Shared-memory hardware")),(0,o.kt)("p",null,'This term refers to a physical hardware configuration in which a\nsingle shared-memory address space is accessible by multiple CPU\ncores; each CPU core "shares" the memory with the other cores.\nA common example of a shared-memory system is a Windows\ndesktop machine or workstation with one or two multicore processors.'),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Distributed-memory hardware")),(0,o.kt)("p",null,"This term refers to a physical hardware configuration in which\nmultiple machines are connected together on a network (that is,\na cluster). Each machine on the network (that is, each compute\nnode on the cluster) has its own memory address space. Communication\nbetween machines is handled by interconnects (Gigabit\nEthernet, Infiniband, etc.)."),(0,o.kt)("p",null,"Virtually all clusters involve both shared-memory and distributedmemory\nhardware. Each compute node on the cluster typically\ncontains at least two or more CPU cores, which means there is a\nshared-memory environment within a compute node. The distributed-\nmemory environment requires communication between the\ncompute nodes involved in the cluster."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"GPU hardware")),(0,o.kt)("p",null,"A graphics processing unit (GPU) is a specialized microprocessor\nthat off-loads and accelerates graphics rendering from the microprocessor.\nTheir highly parallel structure makes GPUs more effective\nthan general-purpose CPUs for a range of complex algorithms. In\na personal computer, a GPU on a dedicated video card is more\npowerful than a GPU that is integrated on the motherboard."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Head compute node")),(0,o.kt)("p",null,"In a Distributed ANSYS run, the machine or node on which the\nmaster process runs (that is, the machine on which the job is\nlaunched). The head compute node should not be confused with\nthe host node in a Windows cluster environment. The host node\ntypically schedules multiple applications and jobs on a cluster,\nbut does not typically run the application.")),(0,o.kt)(s.Z,{value:"software",mdxType:"TabItem"},(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Software Terminology")),(0,o.kt)("p",null,"The following terms describe our software offerings for parallel processing:"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Shared-memory ANSYS")),(0,o.kt)("p",null,"This term refers to running across multiple cores on a single machine\n(for example, a desktop workstation or a single compute\nnode of a cluster). Shared-memory parallelism is invoked, which\nallows each core involved to share data (or memory) as needed\nto perform the necessary parallel computations.When run within\na shared-memory architecture, most computations in the solution\nphase and many pre- and postprocessing operations are performed\nin parallel. For more information, see Using Shared-Memory ANSYS\n(p. 5)."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Distributed ANSYS")," "),(0,o.kt)("p",null,"This term refers to running across multiple cores on a single machine\n(for example, a desktop workstation or a single compute\nnode of a cluster) or across multiple machines (for example, a\ncluster). Distributed-memory parallelism is invoked, and each core\ncommunicates data needed to perform the necessary parallel\ncomputations through the use of MPI (Message Passing Interface)\nsoftware.With Distributed ANSYS, all computations in the solution\nphase are performed in parallel (including the stiffness matrix\ngeneration, linear equation solving, and results calculations). Preand\npostprocessing do not make use of the distributed-memory\nparallel processing; however, these steps can make use of sharedmemory\nparallelism. See Using Distributed ANSYS (p. 17) for more\ndetails."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"GPU accelerator capability")," "),(0,o.kt)("p",null,"This capability takes advantage of the highly parallel architecture\nof the GPU hardware to accelerate the speed of solver computations\nand, therefore, reduce the time required to complete a simulation.\nSome computations of certain equation solvers can be\noff-loaded from the CPU(s) to the GPU, where they are often executed\nmuch faster. The CPU core(s) will continue to be used for\nall other computations in and around the equation solvers. For\nmore information, see GPU Accelerator Capability (p. 9)."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Master process")," "),(0,o.kt)("p",null,"The first process launched on the head compute node in a Distributed\nANSYS run."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Worker process")," "),(0,o.kt)("p",null,"A Distributed ANSYS process other than the master process."),(0,o.kt)("p",null,"Shared-memory ANSYS can only be run on shared-memory hardware. However, Distributed ANSYS\ncan be run on both shared-memory hardware or distributed-memory hardware.While both forms of\nhardware can achieve a significant speedup with Distributed ANSYS, only running on distributedmemory\nhardware allows you to take advantage of increased resources (for example, available memory\nand disk space, as well as memory and I/O bandwidths) by using multiple machines. The GPU accelerator\ncapability can be used with either shared-memory ANSYS or Distributed ANSYS."))),(0,o.kt)("h3",{id:"12-hpc-licensing"},"1.2 HPC Licensing"),(0,o.kt)("p",null,"ANSYS, Inc. offers the following high performance computing license options:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"ANSYS HPC")," - These physics-neutral licenses can be used to run a single analysis across multiple\nprocessors (cores).")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"ANSYS HPC Packs")," - These physics-neutral licenses share the same characteristics of the ANSYS HPC\nlicenses, but are combined into predefined packs to give you greater value and scalability."))),(0,o.kt)("p",null,"For detailed information on these HPC license options, see HPC Licensing in the ANSYS Licensing Guide."),(0,o.kt)("p",null,"The HPC license options cannot be combined with each other in a single solution; for example, you\ncannot use both ANSYS HPC and ANSYS HPC Packs in the same analysis solution."),(0,o.kt)("p",null,"The order in which HPC licenses are used is specified by your user license preferences setting. See\nSpecify Product Order in the ANSYS Licensing Guide for more information on setting user license product\norder."),(0,o.kt)("p",null,"You can choose a particular HPC license by using the Preferred Parallel Feature command line option.\nThe format is ",(0,o.kt)("inlineCode",{parentName:"p"},"ansys211 -ppf <license feature name>"),", where ",(0,o.kt)("inlineCode",{parentName:"p"},"<license feature name>"),"\nis the name of the HPC license option that you want to use. This option forces Mechanical APDL to use\nthe specified license feature for the requested number of parallel cores or GPUs. If the license feature\nis entered incorrectly or the license feature is not available, a license failure occurs."),(0,o.kt)("p",null,"Both Distributed ANSYS and shared-memory ANSYS allow you to use four CPU cores without using any\nHPC licenses. ANSYS HPC licenses add cores to this base functionality, while the ANSYS HPC Pack licenses\nfunction independently of the four included cores."),(0,o.kt)("p",null,"In a similar way, you can use up to four CPU cores and GPUs combined without any HPC licensing (for\nexample, one CPU and three GPUs). The combined number of CPU cores and GPUs used cannot exceed\nthe task limit allowed by your specific license configuration."),(0,o.kt)("h2",{id:"chapter-2-using-shared-memory-ansys"},"Chapter 2: Using Shared-Memory ANSYS ",(0,o.kt)("a",{name:"sharedmemory"})),(0,o.kt)("p",null,"When running a simulation, the solution time is typically dominated by three main parts: ",(0,o.kt)("em",{parentName:"p"},"the time spent\nto create the element matrices and form the global matrices, the time to solve the linear system of\nequations, and the time spent calculating derived quantities (such as stress and strain) and other requested\nresults for each element.")),(0,o.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"IMPORTANT")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Shared-memory ANSYS can run a solution over multiple cores on a single machine.When using sharedmemory\nparallel processing, you can reduce each of the three main parts of the overall solution time\nby using multiple cores. However, this approach is often limited by the memory bandwidth; ",(0,o.kt)("strong",{parentName:"p"},"you typically\nsee very little reduction in solution time beyond four cores.")))),(0,o.kt)("p",null,"The main program functions that run in parallel on shared-memory hardware are:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Solvers such as the Sparse, PCG, ICCG, Block Lanczos, PCG Lanczos, Supernode, and Subspace running\nover multiple processors but sharing the same memory address. These solvers typically have limited\nscalability when used with shared-memory parallelism. In general, very little reduction in time occurs\nwhen using more than four cores."),(0,o.kt)("li",{parentName:"ul"},"Forming element matrices and load vectors."),(0,o.kt)("li",{parentName:"ul"},"Computing derived quantities and other requested results for each element."),(0,o.kt)("li",{parentName:"ul"},"Pre- and postprocessing functions such as graphics, selecting, sorting, and other data and compute\nintensive operations.")),(0,o.kt)("h3",{id:"21-activating-parallel-processing-in-a-shared-memory-architecture"},"2.1 Activating Parallel Processing in a Shared-Memory Architecture"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"By default, shared-memory ANSYS uses two cores and does not require any HPC licenses. Additional\nHPC licenses are required to run with more than four cores.")," Several HPC license options are available.\nSee HPC Licensing (p. 3) for more information.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Open the Mechanical APDL Product Launcher:"))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Windows: ",(0,o.kt)("inlineCode",{parentName:"li"},"Start >Programs >ANSYS 2021 R1 >Mechanical APDL Product Launcher")),(0,o.kt)("li",{parentName:"ul"},"Linux: ",(0,o.kt)("inlineCode",{parentName:"li"},"launcher211"))),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Select the correct environment and license.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Go to the ",(0,o.kt)("inlineCode",{parentName:"p"},"High Performance Computing Setup tab"),". Select ",(0,o.kt)("inlineCode",{parentName:"p"},"Use Shared-Memory Parallel (SMP)."),"\nSpecify the number of cores to use.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Alternatively, you can specify the number of cores to use via the -np command line option:"))),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"ansys211 -smp -np N")),(0,o.kt)("p",null,"where ",(0,o.kt)("em",{parentName:"p"},"N")," represents the number of cores to use."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"For large multiprocessor servers, ANSYS, Inc. recommends setting ",(0,o.kt)("em",{parentName:"strong"},"N")," to a value no higher than the\nnumber of available cores minus one. For example, on an eight-core system, set N to 7. However,\non multiprocessor workstations, you may want to use all available cores to minimize the total\nsolution time. The program automatically limits the maximum number of cores used to be less\nthan or equal to the number of physical cores on the machine. This is done to avoid running the\nprogram on virtual cores (for example, by means of hyperthreading), which typically results in poor\nper-core performance. For optimal performance, consider closing down all other applications before\nlaunching ANSYS.")),(0,o.kt)("p",null,"If you have more than one HPC license feature, you can use the -ppf command line option to\nspecify which HPC license to use for the parallel run. See HPC Licensing (p. 3) for more information."),(0,o.kt)("ol",{start:6},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"If working from the launcher, click Run to launch ANSYS.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Set up and run your analysis as you normally would."))),(0,o.kt)("h3",{id:"211-system-specific-considerations"},"2.1.1 System Specific Considerations"),(0,o.kt)("p",null,"For shared-memory parallel processing, the number of cores that the program uses is limited to the\nlesser of one of the following:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The number of ANSYS HPC licenses available (plus the first four cores which do not require any licenses)"),(0,o.kt)("li",{parentName:"ul"},"The number of cores indicated via the -np command line argument"),(0,o.kt)("li",{parentName:"ul"},"The actual number of cores available")),(0,o.kt)("p",null,"You can specify multiple settings for the number of cores to use during a session. However, ANSYS,\nInc. recommends that you issue the ",(0,o.kt)("strong",{parentName:"p"},"/CLEAR")," command before resetting the number of cores for\nsubsequent analyses."),(0,o.kt)("h3",{id:"22-troubleshooting"},"2.2 Troubleshooting"),(0,o.kt)("p",null,"This section describes problems which you may encounter while using shared-memory parallel processing\nas well as methods for overcoming these problems. Some of these problems are specific to a particular\nsystem, as noted."),(0,o.kt)(i.Z,{defaultValue:"sigterm",values:[{label:"Job Failes with SIGTERM signal (Linux Only)",value:"sigterm"},{label:"Poor Speedup or No Speedup",value:"speed"},{label:"Different Results Relative to a Single Core",value:"singlecore"}],mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"sigterm",mdxType:"TabItem"},(0,o.kt)("p",null,"Occasionally, when running on Linux, a simulation may fail with the following message: \u201cprocess\nkilled (SIGTERM)\u201d. This typically occurs when computing the solution and means that the system\nhas killed the ANSYS process. The two most common occurrences are (1) ANSYS is using too much\nof the hardware resources and the system has killed the ANSYS process or (2) a user has manually\nkilled the ANSYS job (that is, ",(0,o.kt)("strong",{parentName:"p"},"kill -9")," system command). Users should check the size of job they are\nrunning in relation to the amount of physical memory on the machine. Most often, decreasing the\nmodel size or finding a machine with more RAM will result in a successful run.")),(0,o.kt)(s.Z,{value:"speed",mdxType:"TabItem"},"As more cores are utilized, the runtimes are generally expected to decrease. The biggest relative gains are typically achieved when using two cores compared to using a single core.When significant speedups are not seen as additional cores are used, the reasons may involve both hardware and software issues. These include, but are not limited to, the following situations.",(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"HARDWARE")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Oversubscribing hardware"),"   "),(0,o.kt)("p",null,"In a multiuser environment, this could mean that more physical cores are being used by ANSYS\nsimulations than are available on the machine. It could also mean that hyperthreading is activated.\nHyperthreading typically involves enabling extra virtual cores, which can sometimes allow software\nprograms to more effectively use the full processing power of the CPU. However, for computeintensive\nprograms such as ANSYS, using these virtual cores rarely provides a significant reduction\nin runtime. Therefore, it is recommended you disable hyperthreading; if hyperthreading is enabled,\nit is recommended you do not exceed the number of physical cores."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Lack of memory bandwidth"),"\nOn some systems, using most or all of the available cores can result in a lack of memory\nbandwidth. This lack of memory bandwidth can affect the overall scalability of the ANSYS\nsoftware."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Dynamic Processor Speeds")),(0,o.kt)("p",null,"Many new CPUs have the ability to dynamically adjust the clock speed at which they operate\nbased on the current workloads. Typically, when only a single core is being used the clock\nspeed can be significantly higher than when all of the CPU cores are being utilized. This\ncan have a negative effect on scalability as the per-core computational performance can\nbe much higher when only a single core is active versus the case when all of the CPU cores\nare active."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Software")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Simulation includes non-supported features"),"    "),(0,o.kt)("p",null,"The shared- and distributed-memory parallelisms work to speed up certain compute-intensive\noperations in ",(0,o.kt)("strong",{parentName:"p"},"/PREP7, /SOLU")," and ",(0,o.kt)("strong",{parentName:"p"},"/POST1"),". However, not all operations are parallelized. If a\nparticular operation that is not parallelized dominates the simulation time, then using additional\ncores will not help achieve a faster runtime.  "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Simulation has too few DOF (degrees of freedom)")),(0,o.kt)("p",null,"Some analyses (such as transient analyses) may require long compute times, not because\nthe number of DOF is large, but because a large number of calculations are performed (that\nis, a very large number of time steps). Generally, if the number of DOF is relatively small,\nparallel processing will not significantly decrease the solution time. Consequently, for small\nmodels with many time steps, parallel performance may be poor because the model size\nis too small to fully utilize a large number of cores.    "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"I/O cost dominates solution time")),(0,o.kt)("p",null,"For some simulations, the amount of memory required to obtain a solution is greater than\nthe physical memory (that is, RAM) available on the machine. In these cases, either virtual\nmemory (that is, hard disk space) is used by the operating system to hold the data that\nwould otherwise be stored in memory, or the equation solver writes extra files to the disk\nto store data. In both cases, the extra I/O done using the hard drive can significantly affect\nperformance, making the I/O performance the main bottleneck to achieving optimal performance.\nIn these cases, using additional cores will typically not result in a significant reduction\nin overall time to solution.")),(0,o.kt)(s.Z,{value:"singlecore",mdxType:"TabItem"},(0,o.kt)("p",null,"Shared-memory parallel processing occurs in various preprocessing, solution, and postprocessing\noperations. Operational randomness and numerical round-off inherent to parallelism can cause\nslightly different results between runs on the same machine using the same number of cores or\ndifferent numbers of cores. This difference is often negligible. However, in some cases the difference\nis appreciable. This sort of behavior is most commonly seen on nonlinear static or transient analyses\nwhich are numerically unstable. The more numerically unstable the model is, the more likely the\nconvergence pattern or final results will differ as the number of cores used in the simulation is\nchanged."),(0,o.kt)("p",null,"With shared-memory parallelism, you can use the ",(0,o.kt)("strong",{parentName:"p"},"PSCONTROL")," command to control which operations\nactually use parallel behavior. For example, you could use this command to show that the element\nmatrix generation running in parallel is causing a nonlinear job to converge to a slightly different\nsolution each time it runs (even on the same machine with no change to the input data). This can\nhelp isolate parallel computations which are affecting the solution while maintaining as much other\nparallelism as possible to continue to reduce the time to solution."))),(0,o.kt)("h2",{id:"chapter-3-gpu-accelerator-capability"},"Chapter 3: GPU Accelerator Capability ",(0,o.kt)("a",{name:"gpu"})),(0,o.kt)("p",null,"In an effort to provide faster performance during solution, Mechanical APDL supports offloading key\nsolver computations onto graphics cards to accelerate those computations. Only high-end graphics\ncards, the ones with the most amount of cores and memory, can be used to accelerate the solver\ncomputations. For details on which GPU devices are supported and the corresponding driver versions,\nsee the GPU requirements outlined in the Windows Installation Guide and the Linux Installation Guide."),(0,o.kt)("p",null,"It is important to understand that a GPU does not replace the CPU core(s) on which a simulation typically\nruns. One or more CPU cores must be used to run the Mechanical APDL program. The GPUs are used\nin support of the CPU to process certain calculations. The CPU continues to handle most operations\nand will automatically offload some of the time-intensive parallel operations performed by certain\nequation solvers. These parallel solver operations can usually be performed much faster on the highly\nparallel architecture of a GPU, thus accelerating these solvers and reducing the overall time to solution."),(0,o.kt)("p",null,"GPU acceleration can be used with both shared-memory parallel processing (shared-memory ANSYS)\nand distributed-memory parallel processing (Distributed ANSYS). In shared-memory ANSYS, one or\nmultiple GPU accelerator devices can be utilized during solution. In Distributed ANSYS, one or multiple\nGPU accelerator devices per machine or compute node can be utilized during solution."),(0,o.kt)("p",null,"As an example, when using Distributed ANSYS on a cluster involving eight compute nodes with each\ncompute node having two supported GPU accelerator devices, either a single GPU per node (a total of\neight GPU cards) or two GPUs per node (a total of sixteen GPU cards) can be used to accelerate the\nsolution. The GPU accelerator device usage must be consistent across all compute nodes. For example,\nif running a simulation across all compute nodes, it is not possible to use one GPU for some compute\nnodes and zero or two GPUs for the other compute nodes."),(0,o.kt)("p",null,"On machines containing multiple GPU accelerator devices, the program automatically selects the GPU\naccelerator device (or devices) to be used for the simulation. The program cannot detect if a GPU device\nis currently being used by other software, including another Mechanical APDL simulation. Therefore, in\na multiuser environment, users should be careful not to oversubscribe the GPU accelerator devices by\nsimultaneously launching multiple simulations that attempt to use the same GPU (or GPUs) to accelerate\nthe solution. For more information, see Oversubscribing GPU Hardware (p. 14) in the troubleshooting\ndiscussion."),(0,o.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"IMPORTANT")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"The GPU accelerator capability is only supported on the Windows 64-bit and Linux x64 platforms."),(0,o.kt)("p",{parentName:"div"},"You can use up to four GPUs and CPUs combined without any HPC licensing (for example, one CPU\nand three GPUs). To use more than four, you need one or more ANSYS HPC licenses or ANSYS HPC Pack\nlicenses. For more information see HPC Licensing in the ANSYS Licensing Guide."))),(0,o.kt)("h3",{id:"31-activating-the-gpu-accelerator-capability"},"3.1 Activating the GPU Accelerator Capability"),(0,o.kt)("p",null,"Following is the general procedure to use the GPU accelerator capability:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Before activating the GPU accelerator capability, you must have at least one GPU card installed\nwith the proper driver level. You may also need some type of HPC license; see HPC licensing\nfor details.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Open the Mechanical APDL Product Launcher."))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Windows: ",(0,o.kt)("inlineCode",{parentName:"li"},"Start >Programs >ANSYS 2021 R1 >Mechanical APDL Product Launcher")),(0,o.kt)("li",{parentName:"ul"},"Linux: ",(0,o.kt)("inlineCode",{parentName:"li"},"launcher211"))),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Select the correct environment and license.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Go to the ",(0,o.kt)("strong",{parentName:"p"},"High Performance Computing Setup tab"),", select a GPU device from the ",(0,o.kt)("strong",{parentName:"p"},"GPU Accelerator"),"\ndrop-down menu, and specify the number of GPU accelerator devices.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Alternatively, you can activate the GPU accelerator capability via the ",(0,o.kt)("em",{parentName:"p"},"-acc")," command line option:\n",(0,o.kt)("inlineCode",{parentName:"p"},"ansys211 -acc nvidia -na N"),"\nThe ",(0,o.kt)("em",{parentName:"p"},"-na")," command line option followed by a number ",(0,o.kt)("em",{parentName:"p"},"(N)")," indicates the number of GPU accelerator\ndevices to use per machine or compute node. If only the ",(0,o.kt)("em",{parentName:"p"},"-acc")," option is specified, the\nprogram uses a single GPU device per machine or compute node by default (that is, ",(0,o.kt)("em",{parentName:"p"},"-na 1"),")."))),(0,o.kt)("p",null,"If you have more than one HPC license feature, you can use the ",(0,o.kt)("em",{parentName:"p"},"-ppf")," command line option\nto specify which HPC license to use for the parallel run. See HPC Licensing (p. 3) for more information."),(0,o.kt)("ol",{start:6},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"If working from the launcher, click ",(0,o.kt)("em",{parentName:"p"},"Run")," to launch Mechanical APDL.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Set up and run your analysis as you normally would."))),(0,o.kt)("p",null,"With the GPU accelerator capability, the acceleration obtained by using the parallelism on the GPU\nhardware occurs only during the solution operations. Operational randomness and numerical round-off\ninherent to any parallel algorithm can cause slightly different results between runs on the same machine\nwhen using or not using the GPU hardware to accelerate the simulation."),(0,o.kt)("p",null,"The ",(0,o.kt)("strong",{parentName:"p"},"ACCOPTION")," command can also be used to control activation of the GPU accelerator capability."),(0,o.kt)("h3",{id:"32-supported-analysis-types-and-features"},"3.2 Supported Analysis Types and Features"),(0,o.kt)("p",null,"Some analysis types and features are not supported by the GPU accelerator capability. Supported\nfunctionality also depends on the specified GPU hardware. The following section gives general guidelines\non what is and is not supported."),(0,o.kt)("p",null,"These are not comprehensive lists, but represent major features and capabilities found in the Mechanical\nAPDL program."),(0,o.kt)("h3",{id:"321-nvidia-gpu-hardware"},"3.2.1 NVIDIA GPU Hardware"),(0,o.kt)("p",null,"This section lists analysis capabilities that are supported by the GPU accelerator capability when using\nNVIDIA GPU cards."),(0,o.kt)("h4",{id:"supported-analysis-types"},"Supported Analysis Types"),(0,o.kt)("p",null,"The following analysis types are supported and will use the GPU to accelerate the solution."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Static linear or nonlinear analyses using the sparse, PCG, or JCG solver."),(0,o.kt)("li",{parentName:"ul"},"Buckling analyses using the Block Lanczos or subspace eigensolver."),(0,o.kt)("li",{parentName:"ul"},"Modal analyses using the Block Lanczos, subspace, PCG Lanczos, QR damped, unsymmetric,\nor damped eigensolver."),(0,o.kt)("li",{parentName:"ul"},"Harmonic analyses using the full method and the sparse solver."),(0,o.kt)("li",{parentName:"ul"},"Transient linear or nonlinear analyses using the full method and the sparse, PCG, or JCG\nsolver."),(0,o.kt)("li",{parentName:"ul"},"Substructuring analyses, generation pass only, including the generation pass of component\nmode synthesis (CMS) analyses.")),(0,o.kt)("p",null,"In situations where the analysis type is not supported by the GPU accelerator capability, the solution\nwill continue but GPU acceleration will not be used."),(0,o.kt)("h3",{id:"performance-issues-for-some-solverhardware-combinations"},"Performance Issues for Some Solver/Hardware Combinations"),(0,o.kt)("p",null,"When using the PCG or JCG solver, or the PCG Lanczos eigensolver, any of the recommended NVIDIA\nGPU devices can be expected to achieve good performance."),(0,o.kt)("p",null,"When using the sparse solver or eigensolvers based on the sparse solver (for example, Block Lanczos\nor subspace), only NVIDIA GPU devices with significant double precision performance (FP64) are\nrecommended in order to achieve good performance. For a list of these devices, see the Windows\nInstallation Guide and the Linux Installation Guide."),(0,o.kt)("h3",{id:"shared-memory-parallel-behavior"},"Shared-Memory Parallel Behavior"),(0,o.kt)("p",null,"For the sparse solver (and eigensolvers based on the sparse solver), if one or more GPUs are requested,\nonly a single GPU is used no matter how many are requested."),(0,o.kt)("p",null,"For the PCG and JCG solvers (and eigensolvers based on the PCG solver), all requested GPUs are\nused."),(0,o.kt)("h3",{id:"distributed-memory-parallel-behavior"},"Distributed-Memory Parallel Behavior"),(0,o.kt)("p",null,"For the sparse solver (and eigensolvers based on the sparse solver), if the number of GPUs exceeds\nthe number of processes (the -na value is greater than the -np value on the command line), the\nnumber of GPUs used equals the -np value. If the number of GPUs is less than the number of\nprocesses (-na is less than -np), all requested GPUs are used.\nFor the PCG and JCG solvers (and eigensolvers based on the PCG solver), if the number of GPUs\nexceeds the number of processes (-na is greater than -np), all requested GPUs are used. If the\nnumber of GPUs is less than the number of processes (-na is less than -np), all requested GPUs\nare used."),(0,o.kt)("h3",{id:"3212-supported-features"},"3.2.1.2 Supported Features"),(0,o.kt)("p",null,"As the GPU accelerator capability currently only pertains to the equation solvers, virtually all features\nand element types are supported when using this capability with the supported equation solvers\nlisted in Supported Analysis Types (p. 11). A few limitations exist and are listed below. In these\nsituations, the solution will continue but GPU acceleration will not be used (unless otherwise noted):"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Partial pivoting is activated when using the sparse solver. This most commonly occurs when\nusing current technology elements with mixed u-P formulation, Lagrange multiplier based\nMPC184 elements, Lagrange multiplier based contact elements (TARGE169 through CONTA178),\nor certain circuit elements (CIRCU94, CIRCU124)."),(0,o.kt)("li",{parentName:"ul"},"The memory saving option is activated (MSAVE,ON) when using the PCG solver. In this\nparticular case, the MSAVE option is turned off and GPU acceleration is used."),(0,o.kt)("li",{parentName:"ul"},"Unsymmetric matrices when using the PCG solver."),(0,o.kt)("li",{parentName:"ul"},"A non-supported equation solver is used (for example, ICCG, etc.).")),(0,o.kt)("h3",{id:"33-troubleshooting"},"3.3 Troubleshooting"),(0,o.kt)("p",null,"This section describes problems which you may encounter while using the GPU accelerator capability,\nas well as methods for overcoming these problems. Some of these problems are specific to a particular\nsystem, as noted."),(0,o.kt)("p",null,"NVIDIA GPUs support various compute modes (for example, Exclusive thread, Exclusive process). Only\nthe default compute mode is supported. Using other compute modes may cause the program to fail\nto launch."),(0,o.kt)("p",null,"To list the GPU devices installed on the machine, set the ",(0,o.kt)("strong",{parentName:"p"},"ANSGPU_PRINTDEVICES")," environment variable\nto a value of 1. The printed list may or may not include graphics cards used for display purposes, along\nwith any graphics cards used to accelerate your simulation."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"NO DEVICES")),(0,o.kt)("p",{parentName:"li"},"  Be sure that a recommended GPU device is properly installed and configured. Check the driver level\nto be sure it is current or newer than the driver version supported for your particular device. (See\nthe GPU requirements outlined in the Windows Installation Guide and the Linux Installation Guide.)"),(0,o.kt)("p",{parentName:"li"},"  When using NVIDIA GPU devices, use of the CUDA_VISIBLE_DEVICES environment variable can block\nsome or all of the GPU devices from being visible to the program. Try renaming this environment\nvariable to see if the supported devices can be used."))),(0,o.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Important")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("pre",{parentName:"div"},(0,o.kt)("code",{parentName:"pre"},"On Windows, the use of Remote Desktop may disable the use of a GPU device. Launching\nMechanical APDL through the ANSYS Remote Solve Manager (RSM) when RSM is installed\nas a service may also disable the use of a GPU. In these two scenarios, the GPU Accelerator\nCapability cannot be used. Using the TCC (Tesla Compute Cluster) driver mode, if\napplicable, can circumvent this restriction.\n")))),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"NO VALID DEVICES")),(0,o.kt)("p",{parentName:"li"},"  A GPU device was detected, but it is not a recommended GPU device. Be sure that a recommended\nGPU device is properly installed and configured. Check the driver level to be sure it is current or\nnewer than the supported driver version for your particular device. (See the GPU requirements\noutlined in the Windows Installation Guide and the Linux Installation Guide.) Consider using the\nANSGPU_OVERRIDE environment variable to override the check for valid GPU devices."),(0,o.kt)("p",{parentName:"li"},"  When using NVIDIA GPU devices, use of the CUDA_VISIBLE_DEVICES environment variable can block\nsome or all of the GPU devices from being visible to the program. Try renaming this environment\nvariable to see if the supported devices can be used.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"POOR ACCELERATION OR NO ACCELERATION")),(0,o.kt)("p",{parentName:"li"},"  ",(0,o.kt)("strong",{parentName:"p"},"Simulation includes non-supported features")),(0,o.kt)("p",{parentName:"li"},"  A GPU device will only accelerate certain portions of a simulation, mainly the solution time. If the\nbulk of the simulation time is spent outside of solution, the GPU cannot have a significant effect\non the overall analysis time. Even if the bulk of the simulation is spent inside solution, you must be\nsure that a supported equation solver is utilized during solution and that no unsupported options\nare used. Messages are printed in the output to alert users when a GPU is being used, as well as\nwhen unsupported options/features are chosen which deactivate the GPU accelerator capability."),(0,o.kt)("p",{parentName:"li"},"  ",(0,o.kt)("strong",{parentName:"p"},"Simulation has too few DOF (degrees of freedom)")),(0,o.kt)("p",{parentName:"li"},"  Some analyses (such as transient analyses) may require long compute times, not because the\nnumber of DOF is large, but because a large number of calculations are performed (that is, a\nvery large number of time steps). Generally, if the number of DOF is relatively small, GPU acceleration\nwill not significantly decrease the solution time. Consequently, for small models with\nmany time steps, GPU acceleration may be poor because the model size is too small to fully\nutilize a GPU."),(0,o.kt)("p",{parentName:"li"},"  ",(0,o.kt)("strong",{parentName:"p"},"Simulation does not fully utilize the GPU")),(0,o.kt)("p",{parentName:"li"},"  Only simulations that spend a lot of time performing calculations that are supported on a GPU\ncan expect to see significant speedups when a GPU is used. Only certain computations are\nsupported for GPU acceleration. Therefore, users should check to ensure that a high percentage\nof the solution time was spent performing computations that could possibly be accelerated\non a GPU. This can be done by reviewing the equation solver statistics files as described below.\nSee Measuring Performance in the Performance Guide for more details on the equation solver\nstatistics files."),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("em",{parentName:"p"},"PCG solver file"),": The .PCS file contains statistics for the PCG iterative solver. You should\nfirst check to make sure that the GPU was utilized by the solver. This can be done by\nlooking at the line which begins with: \u201cNumber of cores used\u201d. The string \u201cGPU acceleration\nenabled\u201d will be added to this line if the GPU hardware was used by the solver. If\nthis string is missing, the GPU was not used for that call to the solver. Next, you should\nstudy the elapsed times for both the \u201cPreconditioner Factoring\u201d and \u201cMultiply With A22\u201d\ncomputations. GPU hardware is only used to accelerate these two sets of computations.\nThe wall clock (or elapsed) times for these computations are the areas of interest when\ndetermining how much GPU acceleration is achieved.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("em",{parentName:"p"},"Sparse solver files"),": The .DSP file contains statistics for the sparse direct solver. You\nshould first check to make sure that the GPU was utilized by the solver. This can be\ndone by looking for the following line: \u201cGPU acceleration activated\u201d. This line will be\nprinted if the GPU hardware was used. If this line is missing, the GPU was not used for\nthat call to the solver. Next, you should check the percentage of factorization computations\n(flops) which were accelerated on a GPU. This is shown by the line: \u201cpercentage\nof GPU accelerated flops\u201d. Also, you should look at the time to perform the matrix factorization,\nshown by the line: \u201ctime (cpu & wall) for numeric factor\u201d. GPU hardware is\nonly used to accelerate the matrix factor computations. These lines provide some indication\nof how much GPU acceleration is achieved.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("em",{parentName:"p"},"Eigensolver files"),": The Block Lanczos and Subspace eigensolvers support the use of GPU\ndevices; however, no statistics files are written by these eigensolvers. The .PCS file is\nwritten for the PCG Lanczos eigensolver and can be used as described above for the\nPCG iterative solver."),(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Using multiple GPU devices")),(0,o.kt)("p",{parentName:"li"},"When using the sparse solver in a shared-memory parallel solution, it is expected that running\na simulation with multiple GPU devices will not improve performance compared to running\nwith a single GPU device. In a shared-memory parallel solution, the sparse solver can only make\nuse of one GPU device."),(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Oversubscribing GPU hardware")),(0,o.kt)("p",{parentName:"li"},"The program automatically determines which GPU devices to use. In a multiuser environment,\nthis could mean that one or more of the same GPUs are picked when multiple simulations are\nrun simultaneously, thus oversubscribing the hardware.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"If only a single GPU accelerator device exists in the machine, then only a single user\nshould attempt to make use of it, much in the same way users should avoid oversubscribing\ntheir CPU cores.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"If multiple GPU accelerator devices exist in the machine, you can set the ",(0,o.kt)("strong",{parentName:"p"},"ANSGPU_DEVICE"),"\nenvironment variable, in conjunction with the ",(0,o.kt)("strong",{parentName:"p"},"ANSGPU_PRINTDEVICES")," environment\nvariable mentioned above, to specify which particular GPU accelerator devices to use\nduring the solution."),(0,o.kt)("p",{parentName:"li"},"For example, consider a scenario where ",(0,o.kt)("strong",{parentName:"p"},"ANSGPU_PRINTDEVICES")," shows that four GPU\ndevices are available with device ID values of 1, 3, 5, and 7 respectively, and only the\nsecond and third devices are supported for GPU acceleration. To select only the second\nsupported GPU device, set ",(0,o.kt)("strong",{parentName:"p"},"ANSGPU_DEVICE")," = 5. To select the first and second supported\nGPU devices, set ",(0,o.kt)("strong",{parentName:"p"},"ANSGPU_DEVICE")," = 3:5."),(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Solver/hardware combination")),(0,o.kt)("p",{parentName:"li"},"When using NVIDIA GPU devices, some solvers may not achieve good performance on certain\ndevices. For more information, see Performance Issue for Some Solver/Hardware Combinations\n(p. 11)."))))),(0,o.kt)("h2",{id:"chapter-4-using-distributed-ansys"},"Chapter 4: Using Distributed ANSYS",(0,o.kt)("a",{name:"distributedmemory"})),(0,o.kt)("p",null,"When running a simulation, the solution time is typically dominated by three main parts: the time spent\nto create the element matrices and form the global matrices or global systems of equations, the time\nto solve the linear system of equations, and the time spent calculating derived quantities (such as stress\nand strain) and other requested results for each element."),(0,o.kt)("p",null,"The distributed-memory parallelism offered via Distributed ANSYS allows the entire solution phase to\nrun in parallel, including the stiffness matrix generation, linear equation solving, and results calculations.\nAs a result, a simulation using distributed-memory parallel processing usually achieves much faster\nsolution times than a similar run performed using shared-memory parallel processing (p. 5), particularly\nat higher core counts."),(0,o.kt)("p",null,"Distributed ANSYS can run a solution over multiple cores on a single machine or on multiple machines\n(that is, a cluster). It automatically decomposes the model into smaller domains, transfers the domains\nto each core, solves each domain simultaneously, and creates a complete solution to the model. The\nmemory and disk space required to complete the solution can also be distributed over multiple machines.\nBy utilizing all of the resources of a cluster (computing power, RAM, memory and I/O bandwidth), distributed-\nmemory parallel processing can be used to solve very large problems much more efficiently\ncompared to the same simulation run on a single machine."),(0,o.kt)("h4",{id:"distributed-ansys-behavior"},"Distributed ANSYS Behavior"),(0,o.kt)("p",null,"Distributed ANSYS works by launching multiple ANSYS processes on either a single machine or on\nmultiple machines (as specified by one of the following command line options: -np, -machines, or -\nmpifile). The machine that the distributed run is launched from is referred to as the head compute\nnode, and the other machines are referred to as the compute nodes. The first process launched on the\nhead compute node is referred to as the master process; all other processes are referred to as the\nworker processes."),(0,o.kt)("p",null,"Each Distributed ANSYS process is essentially a running process of shared-memory ANSYS. These processes\nare launched through the specified MPI software layer. The MPI software allows each Distributed ANSYS\nprocess to communicate, or exchange data, with the other processes involved in the distributed simulation."),(0,o.kt)("p",null,"Distributed ANSYS does not currently support all of the analysis types, elements, solution options, etc.\nthat are available with shared-memory ANSYS (see Supported Features (p. 30)). In some cases, Distributed\nANSYS stops the analysis to avoid performing an unsupported action. If this occurs, you must launch\nshared-memory ANSYS to perform the simulation. In other cases, Distributed ANSYS will automatically\ndisable the distributed-memory parallel processing capability and perform the operation using sharedmemory\nparallelism. This disabling of the distributed-memory parallel processing can happen at various\nlevels in the program."),(0,o.kt)("p",null,"The master process handles the inputting of commands as well as all of the pre- and postprocessing\nactions. Only certain commands (for example, the ",(0,o.kt)("strong",{parentName:"p"},"SOLVE")," command and supporting commands such\nas ",(0,o.kt)("strong",{parentName:"p"},"/SOLU, FINISH, /EOF, /EXIT"),", and so on) are communicated to the worker processes for execution."),(0,o.kt)("p",null,"Therefore, outside of the SOLUTION processor (",(0,o.kt)("strong",{parentName:"p"},"/SOLU"),"), Distributed ANSYS behaves very similar to\nshared-memory ANSYS. The master process works on the entire model during these pre- and postprocessing\nsteps and may use shared-memory parallelism to improve performance of these operations.\nDuring this time, the worker processes wait to receive new commands from the master process."),(0,o.kt)("p",null,"Once the ",(0,o.kt)("strong",{parentName:"p"},"SOLVE")," command is issued, it is communicated to the worker processes and all Distributed\nANSYS processes become active. At this time, the program makes a decision as to which mode to use\nwhen computing the solution. In some cases, the solution will proceed using only a distributed-memory\nparallel (DMP) mode. In other cases, similar to pre- and postprocessing, the solution will proceed using\nonly a shared-memory parallel (SMP) mode. In a few cases, a mixed mode may be implemented which\ntries to use as much distributed-memory parallelism as possible for maximum performance. These three\nmodes are described further below."),(0,o.kt)("h4",{id:"pure-dmp-mode"},"Pure DMP Mode"),(0,o.kt)("p",null,"The simulation is fully supported by Distributed ANSYS, and distributed-memory parallelism is used\nthroughout the solution. This mode typically provides optimal performance in Distributed ANSYS."),(0,o.kt)("h4",{id:"mixed-mode"},"Mixed Mode"),(0,o.kt)("p",null,"The simulation involves a particular set of computations that is not supported by Distributed ANSYS.\nExamples include certain equation solvers and remeshing due to mesh nonlinear adaptivity. In\nthese cases, distributed-memory parallelism is used throughout the solution, except for the unsupported\nset of computations.When that step is reached, the worker processes in Distributed ANSYS\nsimply wait while the master process uses shared-memory parallelism to perform the computations.\nAfter the computations are finished, the worker processes continue to compute again until the\nentire solution is completed."),(0,o.kt)("h4",{id:"pure-smp-mode"},"Pure SMP Mode"),(0,o.kt)("p",null,"The simulation involves an analysis type or feature that is not supported by Distributed ANSYS. In\nthis case, distributed-memory parallelism is disabled at the onset of the solution, and sharedmemory\nparallelism is used instead. The worker processes in Distributed ANSYS are not involved\nat all in the solution but simply wait while the master process uses shared-memory parallelism to\ncompute the entire solution."),(0,o.kt)("p",null,"When using shared-memory parallelism inside of Distributed ANSYS (in mixed mode or SMP mode, including\nall pre- and postprocessing operations), the master process will not use more cores on the head\ncompute node than the total cores you specify to be used for the Distributed ANSYS solution. This is\ndone to avoid exceeding the requested CPU resources or the requested number of licenses."),(0,o.kt)("p",null,"The following table shows which steps, including specific equation solvers, can be run in parallel using\nshared-memory ANSYS and Distributed ANSYS."),(0,o.kt)("p",null,"Table 4.1 Parallel Capability in Shared-Memory and Distributed ANSYS"),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Solvers/ Feature"),(0,o.kt)("th",{parentName:"tr",align:"center"},"Shared-Memory ANSYS"),(0,o.kt)("th",{parentName:"tr",align:"center"},"Distributed ANSYS"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Sparse"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"PCG"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"ICCG"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y ","[1]")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"JCG"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y ","[1][2]")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"QMR ","[3]"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y ","[1]")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Block Lanczos eigensolver"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"PCG Lanczos eigensolver"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Supernode eigensolver"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y ","[1]")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Subspace eigensolver"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Unsymmectric eigensolver"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Damped eigensolver"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"QRDAMP eigensolver"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Element formulation, results calculation"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Graphics and other pre- and postprocessing"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y"),(0,o.kt)("td",{parentName:"tr",align:"center"},"Y ","[1]")))),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"This solver/operation only runs in mixed mode.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"For static analyses and transient analyses using the full method (",(0,o.kt)("strong",{parentName:"p"},"TRNOPT"),",FULL), the JCG equation\nsolver runs in pure DMP mode only when the matrix is symmetric. Otherwise, it runs in SMP mode.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"The QMR solver only supports 1 core in SMP mode and in mixed mode."))),(0,o.kt)("p",null,"The maximum number of cores allowed in a Distributed ANSYS analysis is currently set at 8192. Therefore,\nyou can run Distributed ANSYS using anywhere from 2 to 8192 cores (assuming the appropriate HPC\nlicenses are available) for each individual job. Performance results vary widely for every model when\nusing any form of parallel processing. For every model, there is a point where using more cores does\nnot significantly reduce the overall solution time. Therefore, it is expected that most models run in\nDistributed ANSYS can not efficiently make use of hundreds or thousands of cores."),(0,o.kt)("p",null,"Files generated by Distributed ANSYS are named ",(0,o.kt)("em",{parentName:"p"},"Jobnamen.ext"),", where n is the process number. (See\nDifferences in General Behavior (p. 32) for more information.) The master process is always numbered\n0, and the worker processes are 1, 2, etc.When the solution is complete and you issue the ",(0,o.kt)("strong",{parentName:"p"},"FINISH"),"\ncommand in the SOLUTION processor, Distributed ANSYS combines all ",(0,o.kt)("em",{parentName:"p"},"Jobnamen.RST")," files into a\nsingle ",(0,o.kt)("em",{parentName:"p"},"Jobname.RST")," file, located on the head compute node. Other files, such as ",(0,o.kt)("em",{parentName:"p"},".MODE, .ESAV,\n.EMAT"),", etc., may be combined as well upon finishing a distributed solution. (See Differences in Postprocessing\n(p. 37) for more information.)"),(0,o.kt)("p",null,"The remaining sections explain how to configure your environment to run Distributed ANSYS, how to\nrun a Distributed ANSYS analysis, and what features and analysis types are supported in Distributed\nANSYS. You should read these sections carefully and fully understand the process before attempting\nto run a distributed analysis. The proper configuration of your environment and the installation and\nconfiguration of the appropriate MPI software are critical to successfully running a distributed analysis."),(0,o.kt)("h3",{id:"41-configuring-distributed-ansys"},"4.1 Configuring Distributed ANSYS"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"To run Distributed ANSYS on a single machine, no additional setup is required.")),(0,o.kt)("p",null,"To run an analysis with Distributed ANSYS on a cluster, some configuration is required as described in\nthe following sections:"),(0,o.kt)("p",null,"4.1.1. Prerequisites for Running Distributed ANSYS\n4.1.2. Setting Up the Cluster Environment for Distributed ANSYS"),(0,o.kt)("h4",{id:"411-prerequisites-for-running-distributed-ansys"},"4.1.1 Prerequisites for Running Distributed ANSYS"),(0,o.kt)("p",null,"Whether you are running on a single machine or multiple machines, the following condition is true:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"By default, Distributed ANSYS uses two cores and does not require any HPC licenses. Additional licenses\nwill be needed to run a distributed solution with more than four cores. Several HPC license\noptions are available. For more information, see HPC Licensing (p. 3) in the Parallel Processing\nGuide (p. 1).")),(0,o.kt)("p",null,"If you are running on a single machine, there are no additional requirements for running a distributed\nsolution."),(0,o.kt)("p",null,"If you are running across multiple machines (for example, a cluster), your system must meet these\nadditional requirements to run a distributed solution."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Homogeneous network: All machines in the cluster must be the same type, OS level, chip set, and\ninterconnects."),(0,o.kt)("li",{parentName:"ul"},"You must be able to remotely log in to all machines, and all machines in the cluster must have\nidentical directory structures (including the ANSYS 2021 R1 installation, MPI installation, and\nworking directories). Do not change or rename directories after you've launched ANSYS. For more\ninformation, see Directory Structure Across Machines (p. 29) in the Parallel Processing Guide (p. 1)."),(0,o.kt)("li",{parentName:"ul"},"All machines in the cluster must have ANSYS 2021 R1 installed, or must have an NFS mount to the\nANSYS 2021 R1 installation. If not installed on a shared file system, ANSYS 2021 R1 must be installed\nin the same directory path on all systems."),(0,o.kt)("li",{parentName:"ul"},"All machines must have the same version of MPI software installed and running. The table below\nshows the MPI software and version level supported for each platform.")),(0,o.kt)("h4",{id:"4111-mpi-software"},"4.1.1.1 MPI Software"),(0,o.kt)("p",null,"The MPI software supported by Distributed ANSYS depends on the platform (see the table below)."),(0,o.kt)("p",null,"The files needed to run Distributed ANSYS using Intel MPI, MS MPI, or Open MPI are included on\nthe installation media and are installed automatically when you install ANSYS 2021 R1. Therefore,\nwhen running on a single machine (for example, a laptop, a workstation, or a single compute node\nof a cluster) on Windows or Linux, or when running on a Linux cluster, no additional software is\nneeded. However, when running on multiple Windows machines you must use a cluster setup, and\nyou must install the MPI software separately (see Installing the Software (p. 21) later in this section)."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Table 4.2: Platforms and MPI Software")),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Platform"),(0,o.kt)("th",{parentName:"tr",align:null},"MPI Software"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Linux"),(0,o.kt)("td",{parentName:"tr",align:null},"Intel MPI 2018.3.222")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null}),(0,o.kt)("td",{parentName:"tr",align:null},"Open MPI 3.1.5a")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Windows 10 (Single Machine)"),(0,o.kt)("td",{parentName:"tr",align:null},"Intel MPI 2018.3.210")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null}),(0,o.kt)("td",{parentName:"tr",align:null},"MS MPI v10.1.12")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Windows Server 2016 (Cluster)"),(0,o.kt)("td",{parentName:"tr",align:null},"Microsoft HPC Pack (MS MPI v10.1.12)b")))),(0,o.kt)("p",null,"Mellanox OFED driver version 4.4 or higher is required."),(0,o.kt)("p",null,"If you are running Distributed ANSYS across multiple Windows machines, you must use Microsoft HPC Pack (MS MPI) and\nthe HPC Job Manager to start Distributed ANSYS (see Activating Distributed ANSYS (p. 25) )."),(0,o.kt)("h4",{id:"4112-installing-the-software"},"4.1.1.2 Installing the Software"),(0,o.kt)("p",null,"Install ANSYS 2021 R1 following the instructions in the ANSYS, Inc. Installation Guide for your platform.\nBe sure to complete the installation, including all required post-installation procedures."),(0,o.kt)("p",null,"To run Distributed ANSYS on a cluster, you must:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Install ANSYS 2021 R1 on all machines in the cluster, in the exact same location on each\nmachine."),(0,o.kt)("li",{parentName:"ul"},"For Windows, you can use shared drives and symbolic links. Install ANSYS 2021 R1 on one\nWindows machine (for example, C:\\Program Files\\ANSYS Inc\\V211) and then share\nthat installation folder. On the other machines in the cluster, create a symbolic link (at\nC:\\Program Files\\ANSYS Inc\\V211) that points to the UNC path for the shared\nfolder. On Windows systems, you must use the Universal Naming Convention (UNC) for all\nfile and path names for Distributed ANSYS to work correctly."),(0,o.kt)("li",{parentName:"ul"},"For Linux, you can use the exported NFS file systems. Install ANSYS 2021 R1 on one Linux\nmachine (for example, at /ansys_inc/v211), and then export this directory. On the other\nmachines in the cluster, create an NFS mount from the first machine to the same local directory\n(/ansys_inc/v211).")),(0,o.kt)("h4",{id:"installing-mpi-software-on-windows"},"Installing MPI Software on Windows"),(0,o.kt)("p",null,"You can install Intel MPI from the installation launcher by choosing ",(0,o.kt)("strong",{parentName:"p"},"Install MPI for ANSYS, Inc.\nParallel Processing"),". For installation instructions see:"),(0,o.kt)("p",null,"Intel-MPI 2018.3.210 Installation Instructions in the ANSYS, Inc. Installation Guides"),(0,o.kt)("p",null,"Microsoft MPI is installed and ready for use as part of the ANSYS 2021 R1 installation, but if you\nrequire MS MPI on another machine, the installer can be found at C:\\Program Files\\ANSYS\nInc\\V211\\commonfiles\\MPI\\Microsoft\\10.1.12498.18\\Windows\\MSMpiSetup.exe"),(0,o.kt)("h4",{id:"microsoft-hpc-pack-windows-hpc-server-2016"},"Microsoft HPC Pack (Windows HPC Server 2016)"),(0,o.kt)("p",null,"You must complete certain post-installation steps before running Distributed ANSYS on a Microsoft\nHPC Server 2016 system. The post-installation instructions provided below assume that Microsoft\nHPC Server 2016 and Microsoft HPC Pack (which includes MS MPI) are already installed on your\nsystem. The post-installation instructions can be found in the following README files:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"Program Files\\ANSYS Inc\\V211\\commonfiles\\MPI\\WindowsHPC\\README.mht"),"\nor\n",(0,o.kt)("inlineCode",{parentName:"p"},"Program Files\\ANSYS Inc\\V211\\commonfiles\\MPI\\WindowsHPC\\README.docx")),(0,o.kt)("p",null,"Microsoft HPC Pack examples are also located in ",(0,o.kt)("em",{parentName:"p"},"Program Files\\ANSYS Inc\\V211\\commonfiles\\\nMPI\\WindowsHPC"),". Jobs are submitted to the Microsoft HPC Job Manager either from the\ncommand line or the Job Manager GUI."),(0,o.kt)("p",null,"To submit a job via the GUI, go to ",(0,o.kt)("strong",{parentName:"p"},"Start> All Programs> Microsoft HPC Pack> HPC Job Manager"),".\nThen click on ",(0,o.kt)("strong",{parentName:"p"},"Create New Job from Description File.")),(0,o.kt)("h4",{id:"412-setting-up-the-cluster-environment-for-distributed-ansys"},"4.1.2. Setting up the Cluster Environment for Distributed ANSYS"),(0,o.kt)("p",null,"After you've ensured that your cluster meets the prerequisites and you have ANSYS 2021 R1 and the\ncorrect version of MPI installed, you need to configure your distributed environment using the following\nprocedure."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Obtain the machine name for each machine on the cluster.")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Windows 10 and Windows Server 2016:")),(0,o.kt)("p",{parentName:"li"},"From the Start menu, pick Settings >System >About. The full computer name is listed\nunder PC Name. Note the name of each machine (not including the domain).")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Linux:"),"\nType ",(0,o.kt)("inlineCode",{parentName:"p"},"hostname")," on each machine in the cluster. Note the name of each machine."))),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"Linux only"),": First determine if the cluster uses the secure shell (ssh) or remote shell (rsh) protocol.")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"For ssh: Use the ssh-keygen command to generate a pair of authentication keys. Do not\nenter a passphrase. Then append the new public key to the list of authorized keys on\neach compute node in the cluster that you wish to use.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"For rsh: Create a .rhosts file in the home directory. Add the name of each compute\nnode you wish to use on a separate line in the .rhosts file. Change the permissions of\nthe .rhost file by issuing: ",(0,o.kt)("strong",{parentName:"p"},"chmod 600 .rhosts"),". Copy this .rhosts file to the home\ndirectory on each compute node in the cluster you wish to use."))),(0,o.kt)("p",null,'Verify communication between compute nodes on the cluster via ssh or rsh. You should not be\nprompted for a password. If you are, correct this before continuing. For more information on\nusing ssh/rsh without passwords, search online for "Passwordless SSH" or "Passwordless RSH", or\nsee the man pages for ssh or rsh.'),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"Windows only"),": Verify that all required environment variables are properly set. If you followed\nthe post-installation instructions described above for Microsoft HPC Pack (Windows HPC Server),\nthese variables should be set automatically.")),(0,o.kt)("p",null,"On the head compute node, where ANSYS 2021 R1 is installed, check these variables:"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"ANSYS211_DIR"),"=C:\\Program Files\\ANSYS Inc\\v211\\ansys\n",(0,o.kt)("strong",{parentName:"p"},"ANSYSLIC_DIR"),"=C:\\Program Files\\ANSYS Inc\\Shared Files\\Licensing"),(0,o.kt)("p",null,"where ",(0,o.kt)("inlineCode",{parentName:"p"},"C:\\Program Files\\ANSYS Inc")," is the location of the product install and ",(0,o.kt)("inlineCode",{parentName:"p"},"C:\\Program\nFiles\\ANSYS Inc\\Shared Files\\Licensing")," is the location of the licensing install. If\nyour installation locations are different than these, specify those paths instead."),(0,o.kt)("p",null,"On Windows systems, you must use the Universal Naming Convention (UNC) for all ANSYS, Inc.\nenvironment variables on the compute nodes for Distributed ANSYS to work correctly."),(0,o.kt)("p",null,"On the compute nodes, check these variables:"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"ANSYS211_DIR"),"=","\\","head_node_machine_name\\ANSYS Inc\\v211\\ansys\n",(0,o.kt)("strong",{parentName:"p"},"ANSYSLIC_DIR"),"=","\\","head_node_machine_name\\ANSYS Inc\\Shared Files\\Licensing"),(0,o.kt)("ol",{start:4},(0,o.kt)("li",{parentName:"ol"},"Windows only: Share out the ANSYS Inc directory on the head node with full permissions so\nthat the compute nodes can access it.")))}d.isMDXComponent=!0},6010:function(e,t,n){function a(e){var t,n,r="";if("string"==typeof e||"number"==typeof e)r+=e;else if("object"==typeof e)if(Array.isArray(e))for(t=0;t<e.length;t++)e[t]&&(n=a(e[t]))&&(r&&(r+=" "),r+=n);else for(t in e)e[t]&&(r&&(r+=" "),r+=t);return r}function r(){for(var e,t,n=0,r="";n<arguments.length;)(e=arguments[n++])&&(t=a(e))&&(r&&(r+=" "),r+=t);return r}n.d(t,{Z:function(){return r}})}}]);